{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09f97d56-2e3e-41dd-9446-bdac1f360a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from nlp_datasets import YahooDataset\n",
    "from nlp_datasets import BaseDataset\n",
    "from nlp_modeltrainers import BaseTrainerModule\n",
    "from nlp_modeltrainers.sentence_classification import MulticlassSentenceClassificationTrainerModule\n",
    "# from nlp_modeltrainers import VectorCosineSimilarityTrainerModule\n",
    "\n",
    "\n",
    "import torch\n",
    "import fastwer\n",
    "from string import ascii_letters as letters\n",
    "L = list(letters)\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.nn import Module, Linear, Embedding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "dev = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8030afd-867a-4214-9d3b-78e0666849ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.SubwordHash import subwordhash\n",
    "from utils.SubwordEmbedding import subwordembedding\n",
    "from utils.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d12cddf4-2313-48c1-814f-22534fa6c258",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Hash_Preprocessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        tokenized = word_tokenize(sample[\"input\"])\n",
    "        tokenized_hashes = self.hash_tokenize(tokenized)\n",
    "        output_id = self.padding(tokenized_hashes, padding_idx=0)\n",
    "        \n",
    "        return {\"input\": output_id, \"target\": sample['target']-1}\n",
    "    \n",
    "    def hash_tokenize(self, data):\n",
    "        tokenized_id = [subword_hashes(w) for w in data]\n",
    "        return tokenized_id\n",
    "    \n",
    "    def padding(self, data, padding_idx=0):\n",
    "        if len(data) >= max_sample_len:\n",
    "            return torch.tensor(data[:max_sample_len], dtype = torch.long).to(device)\n",
    "        data.extend(np.array([[padding_idx]*max_sw_hash_len]*(max_sample_len - len(data))))\n",
    "        return torch.tensor(data, dtype = torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0010c7c7-a63b-485f-8812-2cdc5e4be5ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Misspell_Preprocessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        random_freq = True\n",
    "        tokenized = {i:w for i,w in enumerate(word_tokenize(sample[\"input\"]))}\n",
    "        tokenized = self.misspell(tokenized, random_freq)\n",
    "        tokenized_hashes = self.hash_tokenize(tokenized)\n",
    "        output_id = self.padding(tokenized_hashes, padding_idx=0)\n",
    "        \n",
    "        return {\"input\": output_id, \"target\": sample['target']-1}\n",
    "    \n",
    "    def misspell(self, data, random_freq=False):\n",
    "        if random_freq: \n",
    "            msp_f = np.random.uniform(0.1,0.5)\n",
    "        else: \n",
    "            msp_f = misspell_freq\n",
    "        misspell_num = int(len(data)*msp_f)\n",
    "        misspell_idx = np.random.choice(len(data), misspell_num, replace = False)\n",
    "        m_type = {i:mt for i,mt in enumerate(np.random.randint(0, 4, misspell_num))}\n",
    "        m_dict = {0:self.delete, 1:self.insert, 2:self.replace, 3:self.swap}\n",
    "        for i in range(misspell_num):\n",
    "            mp = data[misspell_idx[i]]\n",
    "            if len(mp) > 1:\n",
    "                mp = m_dict[m_type[i]](list(mp))\n",
    "            else:\n",
    "                mp = self.replace(list(mp))\n",
    "            data[misspell_idx[i]] = mp\n",
    "        return [data[w] for w in sorted(data)]\n",
    "    \n",
    "    def delete(self, word):\n",
    "        idx = np.random.randint(len(word))\n",
    "        word.pop(idx)\n",
    "        return ''.join(map(str,word))\n",
    "\n",
    "    def insert(self, word):\n",
    "        idx = np.random.randint(len(word))\n",
    "        letter = np.random.choice(L)\n",
    "        word.insert(idx, letter)\n",
    "        return ''.join(map(str,word))\n",
    "\n",
    "    def replace(self, word):\n",
    "        idx = np.random.randint(len(word))\n",
    "        letter = np.random.choice(L)\n",
    "        word.pop(idx)\n",
    "        word.insert(idx,letter)\n",
    "        return ''.join(map(str,word))\n",
    "\n",
    "    def swap(self, word):\n",
    "        idx1 = np.random.randint(len(word))\n",
    "        if idx1 == 0:\n",
    "            idx2 = idx1 + 1\n",
    "        elif idx1 == len(word)-1:\n",
    "            idx2 = idx1 -1\n",
    "        else:\n",
    "            idx2 = np.random.choice([idx1+1,idx1-1])\n",
    "        first_idx = min(idx1,idx2)\n",
    "        second_idx = max(idx1,idx2)\n",
    "        temp = word.pop(first_idx)\n",
    "        word.insert(second_idx, temp)\n",
    "        return ''.join(map(str,word))\n",
    "        \n",
    "    def hash_tokenize(self, data):\n",
    "        tokenized_id = [subword_hashes(w) for w in data]\n",
    "        return tokenized_id\n",
    "    \n",
    "    def padding(self, data, padding_idx=0):\n",
    "        if len(data) >= max_sample_len:\n",
    "            return torch.tensor(data[:max_sample_len], dtype = torch.long).to(device)\n",
    "        data.extend(np.array([[padding_idx]*max_sw_hash_len]*(max_sample_len - len(data))))\n",
    "        return torch.tensor(data, dtype = torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afc79c2c-6adb-43d3-b40b-fd03f89fd89c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class YahooClassifier(Module):\n",
    "    def __init__(self, word_embedding, embedding_dim, class_zize,device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = word_embedding.to(device)\n",
    "        self.linear_classifier = Linear(embedding_dim, class_size).to(device)\n",
    "    \n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"\n",
    "        toekn_ids: (batch_size, worrd_num, hash_size)\n",
    "        \"\"\"\n",
    "        debug = False\n",
    "        if debug: print(token_ids.shape)\n",
    "        # (batch_size, words_num, embedding_dim)\n",
    "        outputs = self.word_embedding(token_ids).to(self.device)\n",
    "        if debug: print(outputs.shape)\n",
    "        # (batch_size, embedding_dim)\n",
    "        outputs = torch.max(outputs, dim=1)[0].to(self.device)\n",
    "        if debug: print(outputs.shape)\n",
    "        # (batch_size, class_size)\n",
    "        outputs = self.linear_classifier(outputs).to(self.device)\n",
    "        if debug: print(outputs.shape)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5fbd72f-ada6-45b3-bc2c-77fbad31811f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 9000/9000 [00:06<00:00, 1300.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377\n",
      "995\n"
     ]
    }
   ],
   "source": [
    "dataset = YahooDataset(max_samples=max_samples, local_dir=\"../datasets/small_yahoo_dataset\")\n",
    "subword_hashes = subwordhash(dataset.train)\n",
    "\n",
    "word_num = subword_hashes.word_num\n",
    "max_sw_hash_len = subword_hashes.max_hash\n",
    "max_sample_len = subword_hashes.max_sample\n",
    "print(max_sw_hash_len)\n",
    "print(max_sample_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a36d172a-0c12-4c95-93fb-855fb8c14b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: ../Trained_Models/SubwordEmbedding/trained_model/trained_model_50d_uniform_100e_3w_latesum\n"
     ]
    }
   ],
   "source": [
    "emb_path = emb_path+'_latesum'\n",
    "word_embedding = subwordembedding(num_embeddings = num_emb, \n",
    "                                  embedding_dim = emb_dim, \n",
    "                                  device = device, \n",
    "                                  padding_idx = 0,\n",
    "                                  sumfirst = True)\n",
    "word_embedding = word_embedding.to(device)\n",
    "word_embedding.load_state_dict(torch.load(emb_path))\n",
    "print(f'Loaded model: {emb_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d14e6f7-8816-4df1-b504-900530f8b9f5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessor = Hash_Preprocessor()\n",
    "\n",
    "dataset.train.set_preprocessor(preprocessor)\n",
    "dataset.val.set_preprocessor(preprocessor)\n",
    "dataset.test.set_preprocessor(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c44af8e-7216-4821-9091-59f7759fab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(dataset.train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset.val, batch_size=batch_size, shuffle=False)\n",
    "dataloader_test = DataLoader(dataset.test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fe9c9b1-b5e6-4721-a8c4-366853bedb8c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "misspell_dataset = YahooDataset(max_samples=max_samples, local_dir=\"../datasets/misspell_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e36ad0ae-9d55-4016-a52c-235e7e1c8de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Misspell_Preprocessor()\n",
    "\n",
    "misspell_dataset.train.set_preprocessor(preprocessor)\n",
    "misspell_dataset.val.set_preprocessor(preprocessor)\n",
    "misspell_dataset.test.set_preprocessor(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30d5b3ff-5515-432a-876e-a2c58e608212",
   "metadata": {},
   "outputs": [],
   "source": [
    "msploader_train = DataLoader(misspell_dataset.train, batch_size=batch_size, shuffle=True)\n",
    "msploader_val = DataLoader(misspell_dataset.val, batch_size=batch_size, shuffle=False)\n",
    "msploader_test = DataLoader(misspell_dataset.test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04171d94-15db-4cf4-b336-5c7ba04bf8c1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "yahoo_classifier = YahooClassifier(word_embedding, emb_dim, class_size,device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07f4bd9d-d261-435b-b7c4-f4762bc84466",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 10])\n",
      "tensor([[ -0.9442,   0.1966,  -4.5950,  -2.9982,   4.1343,   0.9446,  -2.8046,\n",
      "          -0.6334,  -3.0932,   1.0603],\n",
      "        [ -0.1649,  -3.5567,  -4.5437,  -4.9015,   5.2856,   0.4494,  -5.8076,\n",
      "          -0.5940,  -3.7902,  -0.6011],\n",
      "        [ -1.9615,  -2.9647,  -2.9552,  -4.6240,   4.6597,   0.2992,  -5.1500,\n",
      "           0.0890,  -3.9151,  -0.2487],\n",
      "        [ -2.9134,  -3.6512,  -6.3358,  -5.8849,   7.3315,  -1.0947,  -8.4590,\n",
      "           0.8159,  -5.4828,  -1.8113],\n",
      "        [ -1.2788,  -0.7232,  -4.4302,  -6.5334,   8.5128,   2.1744,  -3.6169,\n",
      "           1.4120,  -3.7297,  -1.8897],\n",
      "        [  0.1268,  -3.7274,  -7.9826,  -5.5366,   8.2859,   1.3688,  -6.0273,\n",
      "           0.8213,  -5.4673,   0.9888],\n",
      "        [  1.1727,  -3.8323, -12.2453,  -8.1012,  11.3016,   1.4681, -11.2397,\n",
      "           3.7897, -10.1062,  -1.3480],\n",
      "        [ -1.3928,  -1.2304,  -4.4864,  -6.6695,   3.9634,   0.2022,  -4.8462,\n",
      "           0.5809,  -1.8371,  -1.7197]], device='cuda:0',\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader_train:\n",
    "    outputs = yahoo_classifier(batch[\"input\"])\n",
    "    print(outputs.shape)\n",
    "    print(outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1b2f732-c81a-43a3-82fa-b45ab7127324",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier_model = MulticlassSentenceClassificationTrainerModule(yahoo_classifier).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9f38538-962f-4023-bd3f-02aa690d8926",
   "metadata": {},
   "outputs": [],
   "source": [
    "misspell = False\n",
    "if misspell:\n",
    "    logger = pl.loggers.CSVLogger(\"../Trained_Models/Classification/logs\", name = f\"MisspellText_{emb_dim}d\")\n",
    "else:\n",
    "    logger = pl.loggers.CSVLogger(\"../Trained_Models/Classification/logs\", name = f\"CorrectedText_{emb_dim}d\")\n",
    "    \n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath = \"../Trained_Models/Classification/checkpoints\",\n",
    "    filename = 'best_model_{epoch}_{val_loss:.2f}_latesum',\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'min'\n",
    ")\n",
    "class LitProgressBar(pl.callbacks.ProgressBar):\n",
    "    def init_validation_tqdm(self):\n",
    "        bar = tqdm(disable=True)\n",
    "        return bar\n",
    "bar = LitProgressBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "825fe2ff-82d5-45b7-915c-f345ad9011ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\device_parser.py:130: LightningDeprecationWarning: Parsing of the Trainer argument gpus='0' (string) will change in the future. In the current version of Lightning, this will select CUDA device with index 0, but from v1.5 it will select gpus [] (same as gpus=0 (int)).\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | YahooClassifier | 100 M \n",
      "------------------------------------------\n",
      "100 M     Trainable params\n",
      "0         Non-trainable params\n",
      "100 M     Total params\n",
      "400.002   Total estimated model params size (MB)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34cc4f0c99840eca1afd6b674231887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:897: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn('Detected KeyboardInterrupt, attempting graceful shutdown...')\n"
     ]
    }
   ],
   "source": [
    "mode = 'cor'\n",
    "loader = {\n",
    "    'msp':{'train':msploader_train, 'val':msploader_val, 'test':msploader_test},\n",
    "    'cor':{'train':dataloader_train, 'val':dataloader_val, 'test':dataloader_test}\n",
    "}\n",
    "torch.cuda.empty_cache()\n",
    "trainer = pl.Trainer(logger = logger, \n",
    "                     gpus = '0', \n",
    "                     callbacks = [checkpoint, bar], \n",
    "                     num_sanity_val_steps = 0,\n",
    "                     auto_lr_find = True,\n",
    "                     max_epochs = classify_epochs)\n",
    "\n",
    "trainer.fit(classifier_model, \n",
    "            train_dataloader = loader[mode]['train'],\n",
    "            val_dataloaders = loader[mode]['val'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07a6314a-e61a-4720-af86-0ceda3bc83c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "torch.cuda.empty_cache()\n",
    "checkpoint = torch.load(\"./Classification/checkpoints/best_model.ckpt\")\n",
    "# print(checkpoint)\n",
    "w_e = SubwordEmbedding(num_embeddings = num_emb, embedding_dim = emb_dim, device = device, padding_idx = 0)\n",
    "yahoo_classifier = YahooClassifier(word_embedding, emb_dim, class_size,device).to(device)\n",
    "classifier_model = MulticlassSentenceClassificationTrainerModule(yahoo_classifier).to(device)\n",
    "# w_e.load_state_dict(checkpoint['state_dict'])\n",
    "classifier_model.load_state_dict(checkpoint['state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0733471-7933-47f5-94b9-98eb4f091bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f\"../Trained_Models/Classification/trained_model/trained_classification_model_{emb_dim}d_{classify_epochs}e\"\n",
    "torch.save(yahoo_classifier.state_dict(), save_path+'_latesum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce6115f0-ff11-425d-b04c-462433591ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = f\"../Trained_Models/Classification/trained_model/trained_classification_model_{emb_dim}d_{classify_epochs}e\"\n",
    "# classifier = YahooClassifier(word_embedding, emb_dim, class_size,device).to(device)\n",
    "# yahoo_classifier.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f143530-2956-4dd1-a054-b66c4a81a08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe0faf392a64e618c692507462703f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-cf8edbdda734>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self, model, test_dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;31m# run test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m         \u001b[1;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    759\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m         \u001b[1;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mdispatch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluating\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_evaluating\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    796\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_predicting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\accelerators\\accelerator.py\u001b[0m in \u001b[0;36mstart_evaluating\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'pl.Trainer'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_evaluating\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstart_predicting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'pl.Trainer'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py\u001b[0m in \u001b[0;36mstart_evaluating\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'pl.Trainer'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[1;31m# double dispatch to initiate the test loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 148\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_stage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstart_predicting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'pl.Trainer'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mrun_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    804\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluating\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    807\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mrun_evaluate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"run_{self.state.stage}_evaluation\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m             \u001b[0meval_loop_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m         \u001b[1;31m# remove the tensors from the eval results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mrun_evaluation\u001b[1;34m(self, on_epoch)\u001b[0m\n\u001b[0;32m    965\u001b[0m                 \u001b[1;31m# lightning module methods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"evaluation_step_and_end\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m                     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    968\u001b[0m                     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_step_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\evaluation_loop.py\u001b[0m in \u001b[0;36mevaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[0mmodel_ref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_fx_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"test_step\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test_step\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                 \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mmodel_ref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_fx_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"validation_step\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\accelerators\\accelerator.py\u001b[0m in \u001b[0;36mtest_step\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_step_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_step_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mSTEP_OUTPUT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py\u001b[0m in \u001b[0;36mtest_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\nlp_utilities\\nlp_modeltrainers\\src\\nlp_modeltrainers\\BaseTrainerModule.py\u001b[0m in \u001b[0;36mtest_step\u001b[1;34m(self, batch, *args, **kwargs)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcal_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m             \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcal_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"target\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;31m# Record loss and metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\nlp_utilities\\nlp_modeltrainers\\src\\nlp_modeltrainers\\sentence_classification\\MulticlassSentenceClassificationTrainerModule.py\u001b[0m in \u001b[0;36mcal_metrics\u001b[1;34m(self, outputs, targets)\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \"\"\"\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "trainer.test(test_dataloaders = loader[mode]['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf59549-105e-4380-83eb-b9b46354125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(test_dataloaders = loader['msp']['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf297e6-0b55-42b2-a510-76b1682d1a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "b8a566f2-f060-4abc-b33b-afaf9dbacb0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "def a(word):\n",
    "    idx1 = np.random.randint(len(word))\n",
    "    if idx1 == 0:\n",
    "        idx2 = idx1 + 1\n",
    "    elif idx1 == len(word)-1:\n",
    "        idx2 = idx1 -1\n",
    "    else:\n",
    "        idx2 = np.random.choice([idx1+1,idx1-1])\n",
    "    first_idx = min(idx1,idx2)\n",
    "    second_idx = max(idx1,idx2)\n",
    "    temp = word.pop(first_idx)\n",
    "    word.insert(second_idx, temp)\n",
    "    return ''.join(map(str,word))\n",
    "def b(w):\n",
    "    w[2] = 'b'\n",
    "func = {\n",
    "    1:a,\n",
    "    2:b\n",
    "}\n",
    "\n",
    "x = 'strength'\n",
    "y = func[1](list(x))\n",
    "y"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7344240-152d-4b3c-b850-768db014f32d",
   "metadata": {
    "tags": []
   },
   "source": [
    "def p(word):\n",
    "    word[0] = 10\n",
    "\n",
    "a = [1,2,3,4]\n",
    "p(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d1820fc-2cba-4c30-b8dd-3da5b4f8966f",
   "metadata": {
    "tags": []
   },
   "source": [
    "def misspell(data):\n",
    "    random_freq = False\n",
    "    if random_freq: \n",
    "        msp_f = np.random.uniform(0.1,0.5)\n",
    "    else: \n",
    "        msp_f = misspell_freq\n",
    "    misspell_num = int(len(data)*msp_f)\n",
    "    misspell_idx = np.random.choice(len(data), misspell_num, replace = False)\n",
    "    m_type = {i:mt for i,mt in enumerate(np.random.randint(0, 4, misspell_num))}\n",
    "    m_dict = {0:delete, 1:insert, 2:replace, 3:swap}\n",
    "    for i in range(misspell_num):\n",
    "        mp = data[misspell_idx[i]]\n",
    "        mp2 = mp[:]\n",
    "        if len(mp) > 1:\n",
    "            mp2 = m_dict[m_type[i]](list(mp))\n",
    "        else:\n",
    "            mp2 = replace(list(mp))\n",
    "        print(f'{mp} : {mp2}')\n",
    "\n",
    "def delete(word):\n",
    "    idx = np.random.randint(len(word))\n",
    "    word.pop(idx)\n",
    "    return ''.join(map(str,word))\n",
    "\n",
    "def insert(word):\n",
    "    idx = np.random.randint(len(word))\n",
    "    letter = np.random.choice(L)\n",
    "    word.insert(idx, letter)\n",
    "    return ''.join(map(str,word))\n",
    "\n",
    "def replace(word):\n",
    "    idx = np.random.randint(len(word))\n",
    "    letter = np.random.choice(L)\n",
    "    word.pop(idx)\n",
    "    word.insert(idx,letter)\n",
    "    return ''.join(map(str,word))\n",
    "\n",
    "def swap(word):\n",
    "    idx1 = np.random.randint(len(word))\n",
    "    if idx1 == 0:\n",
    "        idx2 = idx1 + 1\n",
    "    elif idx1 == len(word)-1:\n",
    "        idx2 = idx1 -1\n",
    "    else:\n",
    "        idx2 = np.random.choice([idx1+1,idx1-1])\n",
    "    first_idx = min(idx1,idx2)\n",
    "    second_idx = max(idx1,idx2)\n",
    "    temp = word.pop(first_idx)\n",
    "    word.insert(second_idx, temp)\n",
    "    return ''.join(map(str,word))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df4789e6-8af6-4124-b7d6-2c4c1b2e4420",
   "metadata": {
    "tags": []
   },
   "source": [
    "sample = word_tokenize('Fuck your bitch-ass horny jail. Can a man get a fucking hard-on without being assaulted by a dumbass looking dog bitch holding a png ass baseball bat over the fucking internet? Can a male have a crumb or two of libido without being “bonked” and sent to “horny jail”? Can I have a fucking natural human instinct without being fucking beaten and imprisoned by this little bitch boy Cheems? “Oh goody this man is being an average person expressing his lust over his screen where he feels safe to share it? Luckily I have ‘Bonk, go to horny jail’ permanently saved to my fucking clipboard!” Fuck off damnit. Let a guy be horny. Let a guy be in heat. Let a guy feel sexual attraction to a female for fuck’s sake. Slap yourself across the face. Wake yourself up from the sad life you lead, preventing strangers on the internet from being happily horny behind their screens. Wake the fuck up from your “job” at r/bonkpatrol and make a fucking living for once you fucking anti-lust cretin. Go the fuck outside, maybe you can beat couples with baseball bats and get arrested out there. You’d serve a greater purpose to society in a cage than in your rgb room with a high-end pc to harass internet people with. Go fucking retard jail you piss goblin')\n",
    "# print(sample)\n",
    "misspell(sample)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d4e51907-0929-43e1-909e-baaf73190174",
   "metadata": {
    "tags": []
   },
   "source": [
    "x = [1,2,30]\n",
    "\n",
    "a = x[2]\n",
    "a = 10\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7248b6ca-f1e1-4de4-8ce6-66602e82259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {True:1, False:2}\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03975f85-af2c-4e4d-a8fc-ac15901e6e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "b = [4,5,6]\n",
    "c = [7,8,9]\n",
    "ld = {\n",
    "    'msp':{'train':a},\n",
    "    'cor':{'train':c}\n",
    "}\n",
    "print(ld['msp']['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1d939d-c7c6-4c0c-932d-ccdc99c1fc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
