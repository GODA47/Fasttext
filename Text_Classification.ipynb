{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09f97d56-2e3e-41dd-9446-bdac1f360a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from nlp_datasets import YahooDataset\n",
    "from nlp_datasets import BaseDataset\n",
    "from nlp_modeltrainers import BaseTrainerModule\n",
    "from nlp_modeltrainers.sentence_classification import MulticlassSentenceClassificationTrainerModule\n",
    "# from nlp_modeltrainers import VectorCosineSimilarityTrainerModule\n",
    "\n",
    "\n",
    "import torch\n",
    "import fastwer\n",
    "from string import ascii_letters as letters\n",
    "L = list(letters.lower())\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.nn import Module, Linear, Embedding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "dev = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8030afd-867a-4214-9d3b-78e0666849ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.SubwordHash import subwordhash, Hash_Preprocessor, Word_Preprocessor\n",
    "from utils.SubwordEmbedding import subwordembedding\n",
    "from utils.config import *\n",
    "from utils.replace_dict import rep\n",
    "from utils.dict_freq import get_freq_dict"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cfcf48df-59df-4f38-853e-c9cfa9fb8d42",
   "metadata": {
    "tags": []
   },
   "source": [
    "class Hash_Preprocessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        tokenized = word_tokenize(sample[\"input\"].lower())\n",
    "        tokenized_hashes = self.hash_tokenize(tokenized)\n",
    "        output_id = self.padding(tokenized_hashes, padding_idx=0)\n",
    "        \n",
    "        return {\"input\": output_id, \"target\": sample['target']-1}\n",
    "    \n",
    "    def hash_tokenize(self, data):\n",
    "        tokenized_id = [subword_hashes(w) for w in data]\n",
    "        return tokenized_id\n",
    "    \n",
    "    def padding(self, data, padding_idx=0):\n",
    "        if len(data) >= max_sample_len:\n",
    "            return torch.tensor(data[:max_sample_len], dtype = torch.long).to(device)\n",
    "        data.extend(np.array([[padding_idx]*max_sw_hash_len]*(max_sample_len - len(data))))\n",
    "        return torch.tensor(data, dtype = torch.long).to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ae71648-8435-491a-b432-8fda4e718299",
   "metadata": {
    "tags": []
   },
   "source": [
    "class Misspell_Preprocessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        random_freq = True\n",
    "        tokenized = {i:w for i,w in enumerate(word_tokenize(sample[\"input\"].lower()))}\n",
    "        tokenized = self.misspell(tokenized, random_freq)\n",
    "        tokenized_hashes = self.hash_tokenize(tokenized)\n",
    "        output_id = self.padding(tokenized_hashes, padding_idx=0)\n",
    "        \n",
    "        return {\"input\": output_id, \"target\": sample['target']-1}\n",
    "    \n",
    "    def misspell(self, data, random_freq=False):\n",
    "        if random_freq: \n",
    "            msp_f = np.random.uniform(0.1,0.5)\n",
    "        else: \n",
    "            msp_f = misspell_freq\n",
    "        misspell_num = int(len(data)*msp_f)\n",
    "        misspell_idx = np.random.choice(len(data), misspell_num, replace = False)\n",
    "        m_type = {i:mt for i,mt in enumerate(np.random.randint(0, 4, misspell_num))}\n",
    "        m_dict = {0:self.delete, 1:self.insert, 2:self.replace, 3:self.swap}\n",
    "        for i in range(misspell_num):\n",
    "            mp = data[misspell_idx[i]]\n",
    "            if len(mp) > 1:\n",
    "                mp = m_dict[m_type[i]](list(mp))\n",
    "            else:\n",
    "                mp = self.replace(list(mp))\n",
    "            data[misspell_idx[i]] = mp\n",
    "        return [data[w] for w in sorted(data)]\n",
    "    \n",
    "    def delete(self, word):\n",
    "        idx = np.random.randint(len(word))\n",
    "        word.pop(idx)\n",
    "        return ''.join(map(str,word))\n",
    "\n",
    "    def insert(self, word):\n",
    "        idx = np.random.randint(len(word))\n",
    "        letter = np.random.choice(L)\n",
    "        word.insert(idx, letter)\n",
    "        return ''.join(map(str,word))\n",
    "\n",
    "    def replace(self, word):\n",
    "        idx = np.random.randint(len(word))\n",
    "        letter = np.random.choice(L)\n",
    "        word.pop(idx)\n",
    "        word.insert(idx,letter)\n",
    "        return ''.join(map(str,word))\n",
    "\n",
    "    def swap(self, word):\n",
    "        idx1 = np.random.randint(len(word))\n",
    "        if idx1 == 0:\n",
    "            idx2 = idx1 + 1\n",
    "        elif idx1 == len(word)-1:\n",
    "            idx2 = idx1 -1\n",
    "        else:\n",
    "            idx2 = np.random.choice([idx1+1,idx1-1])\n",
    "        first_idx = min(idx1,idx2)\n",
    "        second_idx = max(idx1,idx2)\n",
    "        temp = word.pop(first_idx)\n",
    "        word.insert(second_idx, temp)\n",
    "        return ''.join(map(str,word))\n",
    "        \n",
    "    def hash_tokenize(self, data):\n",
    "        tokenized_id = [subword_hashes(w) for w in data]\n",
    "        return tokenized_id\n",
    "    \n",
    "    def padding(self, data, padding_idx=0):\n",
    "        if len(data) >= max_sample_len:\n",
    "            return torch.tensor(data[:max_sample_len], dtype = torch.long).to(device)\n",
    "        data.extend(np.array([[padding_idx]*max_sw_hash_len]*(max_sample_len - len(data))))\n",
    "        return torch.tensor(data, dtype = torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afc79c2c-6adb-43d3-b40b-fd03f89fd89c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class YahooClassifier(Module):\n",
    "    def __init__(self, word_embedding, embedding_dim, class_zize, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = word_embedding.to(device)\n",
    "        self.linear_classifier = Linear(embedding_dim, class_size).to(device)\n",
    "    \n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"\n",
    "        toekn_ids: (batch_size, worrd_num, hash_size)\n",
    "        \"\"\"\n",
    "#         print(token_ids.shape)\n",
    "        # (batch_size, words_num, embedding_dim)\n",
    "        outputs = self.word_embedding(token_ids).to(self.device)\n",
    "#         print(outputs.shape)\n",
    "        # (batch_size, embedding_dim)\n",
    "        outputs = torch.max(outputs, dim=1)[0].to(self.device)\n",
    "#         print(outputs.shape)\n",
    "#         print(outputs)\n",
    "        # (batch_size, class_size)\n",
    "        outputs = self.linear_classifier(outputs).to(self.device)\n",
    "#         print(outputs.shape)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7fe8b07-3731-4220-b525-c22341e167d3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 900/900 [00:02<00:00, 301.60it/s]\n"
     ]
    }
   ],
   "source": [
    "word_dataset = YahooDataset(max_samples=max_samples, local_dir=\"../datasets/small_yahoo_dataset_text\")\n",
    "freq_dict = get_freq_dict(word_dataset.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944c0bd3-c1d3-4e11-aafc-38e18a7fbed0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04c82561-0675-4fc6-bd63-73e1079a0d9f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 900/900 [00:01<00:00, 508.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356\n",
      "811\n"
     ]
    }
   ],
   "source": [
    "train_preprocessor = Word_Preprocessor(freq_dict = freq_dict, \n",
    "                                       train = True)\n",
    "word_dataset.train.set_preprocessor(train_preprocessor)\n",
    "\n",
    "subword_hashes = subwordhash(word_dataset.train)\n",
    "\n",
    "word_num = subword_hashes.word_num\n",
    "max_sw_hash_len = subword_hashes.max_hash\n",
    "max_sample_len = subword_hashes.max_sample\n",
    "print(max_sw_hash_len)\n",
    "print(max_sample_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7120117b-2cd4-432c-83cc-46daff8f1a4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = YahooDataset(max_samples=max_samples, local_dir=\"../datasets/small_yahoo_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a36d172a-0c12-4c95-93fb-855fb8c14b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: ../Trained_Models/SubwordEmbedding/trained_model/trained_model_100d_noisedist_100e_3w_10000sample_min1_culled_noURL_lemmatized\n"
     ]
    }
   ],
   "source": [
    "word_embedding = subwordembedding(num_embeddings = num_emb, embedding_dim = emb_dim, device = device, padding_idx = 0)\n",
    "word_embedding = word_embedding.to(device)\n",
    "word_embedding.load_state_dict(torch.load(emb_path))\n",
    "print(f'Loaded model: {emb_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d14e6f7-8816-4df1-b504-900530f8b9f5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_preprocessor = Hash_Preprocessor(max_sw_hash_len,\n",
    "                                       max_sample_len,\n",
    "                                       subword_hashes,\n",
    "                                       device,\n",
    "                                       freq_dict = freq_dict,\n",
    "                                       train = True,\n",
    "                                       subsampling = subsampling)\n",
    "preprocessor = Hash_Preprocessor(max_sw_hash_len,\n",
    "                                 max_sample_len,\n",
    "                                 subword_hashes,\n",
    "                                 device,\n",
    "                                 freq_dict = freq_dict,\n",
    "                                 train = False,\n",
    "                                 subsampling = subsampling)\n",
    "dataset.train.set_preprocessor(train_preprocessor)\n",
    "dataset.val.set_preprocessor(preprocessor)\n",
    "dataset.test.set_preprocessor(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c44af8e-7216-4821-9091-59f7759fab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(dataset.train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset.val, batch_size=batch_size, shuffle=False)\n",
    "dataloader_test = DataLoader(dataset.test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "72492781-da65-45ad-8db3-524bf2e7e16a",
   "metadata": {
    "tags": []
   },
   "source": [
    "misspell_dataset = YahooDataset(max_samples=max_samples, local_dir=\"../datasets/misspell_dataset\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07d6dad2-2daf-467e-8833-55bbd268c4ed",
   "metadata": {},
   "source": [
    "preprocessor = Misspell_Preprocessor()\n",
    "\n",
    "misspell_dataset.train.set_preprocessor(preprocessor)\n",
    "misspell_dataset.val.set_preprocessor(preprocessor)\n",
    "misspell_dataset.test.set_preprocessor(preprocessor)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "544a8b6f-0c43-4ee0-9986-5ccb5b4638ed",
   "metadata": {},
   "source": [
    "msploader_train = DataLoader(misspell_dataset.train, batch_size=batch_size, shuffle=True)\n",
    "msploader_val = DataLoader(misspell_dataset.val, batch_size=batch_size, shuffle=False)\n",
    "msploader_test = DataLoader(misspell_dataset.test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04171d94-15db-4cf4-b336-5c7ba04bf8c1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "yahoo_classifier = YahooClassifier(word_embedding, emb_dim, class_size,device)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c5d92d3-efa8-4f12-aa96-868490ad612b",
   "metadata": {
    "tags": []
   },
   "source": [
    "for batch in dataloader_train:\n",
    "    outputs = yahoo_classifier(batch[\"input\"])\n",
    "    print(outputs.shape)\n",
    "    print(outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1b2f732-c81a-43a3-82fa-b45ab7127324",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier_model = MulticlassSentenceClassificationTrainerModule(yahoo_classifier).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9f38538-962f-4023-bd3f-02aa690d8926",
   "metadata": {},
   "outputs": [],
   "source": [
    "misspell = False\n",
    "if misspell:\n",
    "    logger = pl.loggers.CSVLogger(\"../Trained_Models/Classification/logs\", name = f\"MisspellText_{emb_dim}d\")\n",
    "else:\n",
    "    logger = pl.loggers.CSVLogger(\"../Trained_Models/Classification/logs\", name = f\"CorrectedText_{emb_dim}d\")\n",
    "    \n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath = \"../Trained_Models/Classification/checkpoints\",\n",
    "    filename = 'best_model_{epoch}_{val_loss}',\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'min'\n",
    ")\n",
    "class LitProgressBar(pl.callbacks.ProgressBar):\n",
    "    def init_validation_tqdm(self):\n",
    "        bar = tqdm(disable=True)\n",
    "        return bar\n",
    "bar = LitProgressBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "825fe2ff-82d5-45b7-915c-f345ad9011ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\device_parser.py:130: LightningDeprecationWarning: Parsing of the Trainer argument gpus='0' (string) will change in the future. In the current version of Lightning, this will select CUDA device with index 0, but from v1.5 it will select gpus [] (same as gpus=0 (int)).\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | YahooClassifier | 200 M \n",
      "------------------------------------------\n",
      "200 M     Trainable params\n",
      "0         Non-trainable params\n",
      "200 M     Total params\n",
      "800.004   Total estimated model params size (MB)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b70753cb158d49ed8c6d99077d6f8812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'max_sample_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-8d56affb4fb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m                      max_epochs = classify_epochs)\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m trainer.fit(classifier_model, \n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mtrain_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             val_dataloaders = loader[mode]['val'])\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[0;32m    458\u001b[0m         )\n\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 460\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    756\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m         \u001b[1;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 758\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    759\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m         \u001b[1;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mdispatch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_predicting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 799\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    800\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    801\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrun_stage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\accelerators\\accelerator.py\u001b[0m in \u001b[0;36mstart_training\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'pl.Trainer'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'pl.Trainer'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'pl.Trainer'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;31m# double dispatch to initiate the training loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_stage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'pl.Trainer'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mrun_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    807\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mrun_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    869\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"run_training_epoch\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m                     \u001b[1;31m# run train epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 871\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    872\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    873\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_steps\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    489\u001b[0m         \u001b[0mis_last_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_last_batch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_last_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_last_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\profiler\\profilers.py\u001b[0m in \u001b[0;36mprofile_iterable\u001b[1;34m(self, iterable, action_name)\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\supporters.py\u001b[0m in \u001b[0;36mprefetch_iterator\u001b[1;34m(iterable)\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 534\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    535\u001b[0m         \u001b[1;31m# yield last and has next\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m         \u001b[1;32myield\u001b[0m \u001b[0mlast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\supporters.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m         \"\"\"\n\u001b[1;32m--> 464\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest_next_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\supporters.py\u001b[0m in \u001b[0;36mrequest_next_batch\u001b[1;34m(loader_iters)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m         \"\"\"\n\u001b[1;32m--> 478\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mapply_to_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader_iters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\apply_func.py\u001b[0m in \u001b[0;36mapply_to_collection\u001b[1;34m(data, dtype, function, wrong_dtype, *args, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;31m# Breaking condition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mwrong_dtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrong_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;31m# Recursively apply to collection items\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\github\\nlp_utilities\\nlp_datasets\\src\\nlp_datasets\\BaseDataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_dirs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Github\\Fasttext\\utils\\SubwordHash.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, sample)\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[0mtokenized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubsample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[0mtokenized_hashes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhash_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m         \u001b[0moutput_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_hashes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"input\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"target\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Github\\Fasttext\\utils\\SubwordHash.py\u001b[0m in \u001b[0;36mpadding\u001b[1;34m(self, data, padding_idx)\u001b[0m\n\u001b[0;32m    175\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_sample_len\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmax_sample_len\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_sw_hash_len\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_sample_len\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'max_sample_len' is not defined"
     ]
    }
   ],
   "source": [
    "mode = 'cor'\n",
    "loader = {\n",
    "#     'msp':{'train':msploader_train, 'val':msploader_val, 'test':msploader_test},\n",
    "    'cor':{'train':dataloader_train, 'val':dataloader_val, 'test':dataloader_test}\n",
    "}\n",
    "torch.cuda.empty_cache()\n",
    "trainer = pl.Trainer(logger = logger, \n",
    "                     gpus = '0', \n",
    "                     callbacks = [checkpoint, bar], \n",
    "                     num_sanity_val_steps = 0,\n",
    "                     auto_lr_find = True,\n",
    "                     max_epochs = classify_epochs)\n",
    "\n",
    "trainer.fit(classifier_model, \n",
    "            train_dataloader = loader[mode]['train'],\n",
    "            val_dataloaders = loader[mode]['val'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07a6314a-e61a-4720-af86-0ceda3bc83c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "torch.cuda.empty_cache()\n",
    "checkpoint = torch.load(\"./Classification/checkpoints/best_model.ckpt\")\n",
    "# print(checkpoint)\n",
    "w_e = SubwordEmbedding(num_embeddings = num_emb, embedding_dim = emb_dim, device = device, padding_idx = 0)\n",
    "yahoo_classifier = YahooClassifier(word_embedding, emb_dim, class_size,device).to(device)\n",
    "classifier_model = MulticlassSentenceClassificationTrainerModule(yahoo_classifier).to(device)\n",
    "# w_e.load_state_dict(checkpoint['state_dict'])\n",
    "classifier_model.load_state_dict(checkpoint['state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0733471-7933-47f5-94b9-98eb4f091bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f\"../Trained_Models/Classification/trained_model/trained_classification_model_{emb_dim}d_{emb_add}\"\n",
    "torch.save(yahoo_classifier.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f143530-2956-4dd1-a054-b66c4a81a08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(test_dataloaders = loader[mode]['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf59549-105e-4380-83eb-b9b46354125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(test_dataloaders = loader['msp']['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf297e6-0b55-42b2-a510-76b1682d1a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "word_tokenize(\"\"\"When's the next friday? 10/12 bbbbBBB Sydney\"\"\".lower())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b8a566f2-f060-4abc-b33b-afaf9dbacb0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "def a(word):\n",
    "    idx1 = np.random.randint(len(word))\n",
    "    if idx1 == 0:\n",
    "        idx2 = idx1 + 1\n",
    "    elif idx1 == len(word)-1:\n",
    "        idx2 = idx1 -1\n",
    "    else:\n",
    "        idx2 = np.random.choice([idx1+1,idx1-1])\n",
    "    first_idx = min(idx1,idx2)\n",
    "    second_idx = max(idx1,idx2)\n",
    "    temp = word.pop(first_idx)\n",
    "    word.insert(second_idx, temp)\n",
    "    return ''.join(map(str,word))\n",
    "def b(w):\n",
    "    w[2] = 'b'\n",
    "func = {\n",
    "    1:a,\n",
    "    2:b\n",
    "}\n",
    "\n",
    "x = 'strength'\n",
    "y = func[1](list(x))\n",
    "y"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7344240-152d-4b3c-b850-768db014f32d",
   "metadata": {
    "tags": []
   },
   "source": [
    "def p(word):\n",
    "    word[0] = 10\n",
    "\n",
    "a = [1,2,3,4]\n",
    "p(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d1820fc-2cba-4c30-b8dd-3da5b4f8966f",
   "metadata": {
    "tags": []
   },
   "source": [
    "def misspell(data):\n",
    "    random_freq = False\n",
    "    if random_freq: \n",
    "        msp_f = np.random.uniform(0.1,0.5)\n",
    "    else: \n",
    "        msp_f = misspell_freq\n",
    "    misspell_num = int(len(data)*msp_f)\n",
    "    misspell_idx = np.random.choice(len(data), misspell_num, replace = False)\n",
    "    m_type = {i:mt for i,mt in enumerate(np.random.randint(0, 4, misspell_num))}\n",
    "    m_dict = {0:delete, 1:insert, 2:replace, 3:swap}\n",
    "    for i in range(misspell_num):\n",
    "        mp = data[misspell_idx[i]]\n",
    "        mp2 = mp[:]\n",
    "        if len(mp) > 1:\n",
    "            mp2 = m_dict[m_type[i]](list(mp))\n",
    "        else:\n",
    "            mp2 = replace(list(mp))\n",
    "        print(f'{mp} : {mp2}')\n",
    "\n",
    "def delete(word):\n",
    "    idx = np.random.randint(len(word))\n",
    "    word.pop(idx)\n",
    "    return ''.join(map(str,word))\n",
    "\n",
    "def insert(word):\n",
    "    idx = np.random.randint(len(word))\n",
    "    letter = np.random.choice(L)\n",
    "    word.insert(idx, letter)\n",
    "    return ''.join(map(str,word))\n",
    "\n",
    "def replace(word):\n",
    "    idx = np.random.randint(len(word))\n",
    "    letter = np.random.choice(L)\n",
    "    word.pop(idx)\n",
    "    word.insert(idx,letter)\n",
    "    return ''.join(map(str,word))\n",
    "\n",
    "def swap(word):\n",
    "    idx1 = np.random.randint(len(word))\n",
    "    if idx1 == 0:\n",
    "        idx2 = idx1 + 1\n",
    "    elif idx1 == len(word)-1:\n",
    "        idx2 = idx1 -1\n",
    "    else:\n",
    "        idx2 = np.random.choice([idx1+1,idx1-1])\n",
    "    first_idx = min(idx1,idx2)\n",
    "    second_idx = max(idx1,idx2)\n",
    "    temp = word.pop(first_idx)\n",
    "    word.insert(second_idx, temp)\n",
    "    return ''.join(map(str,word))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df4789e6-8af6-4124-b7d6-2c4c1b2e4420",
   "metadata": {
    "tags": []
   },
   "source": [
    "sample = word_tokenize('Fuck your bitch-ass horny jail. Can a man get a fucking hard-on without being assaulted by a dumbass looking dog bitch holding a png ass baseball bat over the fucking internet? Can a male have a crumb or two of libido without being “bonked” and sent to “horny jail”? Can I have a fucking natural human instinct without being fucking beaten and imprisoned by this little bitch boy Cheems? “Oh goody this man is being an average person expressing his lust over his screen where he feels safe to share it? Luckily I have ‘Bonk, go to horny jail’ permanently saved to my fucking clipboard!” Fuck off damnit. Let a guy be horny. Let a guy be in heat. Let a guy feel sexual attraction to a female for fuck’s sake. Slap yourself across the face. Wake yourself up from the sad life you lead, preventing strangers on the internet from being happily horny behind their screens. Wake the fuck up from your “job” at r/bonkpatrol and make a fucking living for once you fucking anti-lust cretin. Go the fuck outside, maybe you can beat couples with baseball bats and get arrested out there. You’d serve a greater purpose to society in a cage than in your rgb room with a high-end pc to harass internet people with. Go fucking retard jail you piss goblin')\n",
    "# print(sample)\n",
    "misspell(sample)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d4e51907-0929-43e1-909e-baaf73190174",
   "metadata": {
    "tags": []
   },
   "source": [
    "x = [1,2,30]\n",
    "\n",
    "a = x[2]\n",
    "a = 10\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7248b6ca-f1e1-4de4-8ce6-66602e82259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {True:1, False:2}\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03975f85-af2c-4e4d-a8fc-ac15901e6e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "b = [4,5,6]\n",
    "c = [7,8,9]\n",
    "ld = {\n",
    "    'msp':{'train':a},\n",
    "    'cor':{'train':c}\n",
    "}\n",
    "print(ld['msp']['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1d939d-c7c6-4c0c-932d-ccdc99c1fc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
