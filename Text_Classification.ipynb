{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "292b811e-c7dd-477b-908a-508b3bd4f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# from Utilities.NLP_ModelTrainers.SentenceClassification.MulticlassSentenceClassificationModule import MulticlassSentenceClassificationTrainerModule\n",
    "\n",
    "from nlp_datasets import YahooDataset\n",
    "from nlp_datasets import BaseDataset\n",
    "# from nlp_datasets import SpellingSimilarityDataset\n",
    "from nlp_modeltrainers import BaseTrainerModule\n",
    "from nlp_modeltrainers import MulticlassSentenceClassificationTrainerModule\n",
    "# from nlp_modeltrainers import VectorCosineSimilarityTrainerModule\n",
    "\n",
    "\n",
    "import torch\n",
    "import fastwer\n",
    "from string import ascii_letters as letters\n",
    "L = list(letters)\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.nn import Module, Linear, Embedding, ModuleList\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = 'cpu'\n",
    "\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a6cea0b-b136-4db3-ad7a-7cdc15bba56b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class subwordhash:\n",
    "    def __init__(self, dataset):\n",
    "        word_num, hash_len, sample_len = self.average_subword_num(dataset)\n",
    "        self.word_num = word_num\n",
    "        self.max_hash = hash_len\n",
    "        self.max_sample = sample_len\n",
    "        \n",
    "    def __call__(self, word):\n",
    "        return self.subword_hashes(word, max_hash_num = self.max_hash)\n",
    "    \n",
    "    def fnv1a(self, txt, K = int(2e6 + 1)):\n",
    "        # 64 bit fnv-1a\n",
    "        txt = bytes(txt, 'utf-8')\n",
    "        hval = 0xcbf29ce484222325\n",
    "        fnv_prime = 0x100000001b3\n",
    "        for c in txt:\n",
    "            hval = hval ^ c\n",
    "            hval = (hval * fnv_prime) % K\n",
    "        return hval + 1        \n",
    "\n",
    "    def subword_hashes(self, word, max_hash_num = None, get_len = False):\n",
    "        sub_hash = []\n",
    "        tword = '<' + word + '>'\n",
    "        sub_hash.append(self.fnv1a(tword))\n",
    "        for n in range(3,7):\n",
    "            for i in range(len(tword)-n+1):\n",
    "                sub_hash.append(self.fnv1a(tword[i:i+n]))\n",
    "                if len(sub_hash) == max_hash_num:\n",
    "                    return np.array(sub_hash[:max_hash_num])\n",
    "        if max_hash_num is not None:\n",
    "            sub_hash.extend([0]*(max_hash_num - len(sub_hash)))\n",
    "        if get_len:\n",
    "            return len(sub_hash)\n",
    "        return np.array(sub_hash)\n",
    "\n",
    "    def average_subword_num(self, dataset):\n",
    "        max_sample_len = 0\n",
    "        hash_len_dist = {}\n",
    "        len_dist = {}\n",
    "        for sample in tqdm(dataset):\n",
    "            tokens = word_tokenize(sample[\"input\"])\n",
    "            if len(tokens) not in len_dist:\n",
    "                len_dist[len(tokens)] = 0\n",
    "            len_dist[len(tokens)] += 1\n",
    "            max_sample_len = max(max_sample_len, len(tokens))\n",
    "            \n",
    "        for L in list(len_dist):\n",
    "            hash_len_dist[self.subword_hashes('a'*L, get_len = True)] = len_dist[L]\n",
    "        \n",
    "        total = 0\n",
    "        weighted_hash_len = []\n",
    "        for L in list(hash_len_dist):\n",
    "            total += hash_len_dist[L]\n",
    "            weighted_hash_len.append(hash_len_dist[L]*L)\n",
    "        avg = sum(weighted_hash_len)/total\n",
    "        \n",
    "        return int(total), int(avg), max_sample_len\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "188ad52a-0d92-4e58-87ca-fc8e921b7a21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SubwordEmbedding(Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, device, padding_idx = 0):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.device =  device\n",
    "        self.padding_idx = padding_idx\n",
    "        \n",
    "        self.subword_embedding = Embedding(num_embeddings = num_embeddings, \n",
    "                                           embedding_dim = embedding_dim, \n",
    "                                           padding_idx = padding_idx)\n",
    "    def forward(self, token_ids):\n",
    "        # token_ids: (batch_size, word_num, hash_size)\n",
    "        # return: (batch_size, word_num, embedding_dim)\n",
    "        debug = False\n",
    "        \n",
    "        subword_embed = self.subword_embedding(token_ids).to(self.device)\n",
    "        # (batch_size, word_num, hash_size, embedding_dim)\n",
    "        if debug: print(\"subword_embed.shape: \", subword_embed.shape)\n",
    "        \n",
    "        word_embed = subword_embed.sum(dim = len(subword_embed.shape) -2).to(self.device)\n",
    "        # (batch_size, word_num, embedding_dim)\n",
    "        if debug: print(\"word_embed.shape: \", word_embed.shape)\n",
    "        \n",
    "        if debug: print(\"\\n########################################\\n\")\n",
    "        return word_embed\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d12cddf4-2313-48c1-814f-22534fa6c258",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Hash_Preprocessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        tokenized = word_tokenize(sample[\"input\"])\n",
    "        tokenized_hashes = self.hash_tokenize(tokenized)\n",
    "        output_id = self.padding(tokenized_hashes, padding_idx=0)\n",
    "        \n",
    "        return {\"input\": output_id, \"target\": sample['target']-1}\n",
    "    \n",
    "    def hash_tokenize(self, data):\n",
    "        tokenized_id = [subword_hashes(w) for w in data]\n",
    "        return tokenized_id\n",
    "    \n",
    "    def padding(self, data, padding_idx=0):\n",
    "        if len(data) >= max_sample_len:\n",
    "            return torch.tensor(data[:max_sample_len], dtype = torch.long).to(device)\n",
    "        data.extend(np.array([[padding_idx]*max_sw_hash_len]*(max_sample_len - len(data))))\n",
    "        return torch.tensor(data, dtype = torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0010c7c7-a63b-485f-8812-2cdc5e4be5ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Misspell_Preprocessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        random_freq = True\n",
    "        tokenized = {i:w for i,w in enumerate(word_tokenize(sample[\"input\"]))}\n",
    "        tokenized = self.misspell(tokenized, random_freq)\n",
    "        tokenized_hashes = self.hash_tokenize(tokenized)\n",
    "        output_id = self.padding(tokenized_hashes, padding_idx=0)\n",
    "        \n",
    "        return {\"input\": output_id, \"target\": sample['target']-1}\n",
    "    \n",
    "    def misspell(self, data, random_freq=False):\n",
    "        if random_freq: \n",
    "            msp_f = np.random.uniform(0.1,0.5)\n",
    "        else: \n",
    "            msp_f = misspell_freq\n",
    "        misspell_num = int(len(data)*msp_f)\n",
    "        misspell_idx = np.random.choice(len(data), misspell_num, replace = False)\n",
    "        m_type = {i:mt for i,mt in enumerate(np.random.randint(0, 4, misspell_num))}\n",
    "        m_dict = {0:self.delete, 1:self.insert, 2:self.replace, 3:self.swap}\n",
    "        for i in range(misspell_num):\n",
    "            mp = data[misspell_idx[i]]\n",
    "            if len(mp) > 1:\n",
    "                mp = m_dict[m_type[i]](list(mp))\n",
    "            else:\n",
    "                mp = self.replace(list(mp))\n",
    "            data[misspell_idx[i]] = mp\n",
    "        return [data[w] for w in sorted(data)]\n",
    "    \n",
    "    def delete(self, word):\n",
    "        idx = np.random.randint(len(word))\n",
    "        word.pop(idx)\n",
    "        return ''.join(map(str,word))\n",
    "\n",
    "    def insert(self, word):\n",
    "        idx = np.random.randint(len(word))\n",
    "        letter = np.random.choice(L)\n",
    "        word.insert(idx, letter)\n",
    "        return ''.join(map(str,word))\n",
    "\n",
    "    def replace(self, word):\n",
    "        idx = np.random.randint(len(word))\n",
    "        letter = np.random.choice(L)\n",
    "        word.pop(idx)\n",
    "        word.insert(idx,letter)\n",
    "        return ''.join(map(str,word))\n",
    "\n",
    "    def swap(self, word):\n",
    "        idx1 = np.random.randint(len(word))\n",
    "        if idx1 == 0:\n",
    "            idx2 = idx1 + 1\n",
    "        elif idx1 == len(word)-1:\n",
    "            idx2 = idx1 -1\n",
    "        else:\n",
    "            idx2 = np.random.choice([idx1+1,idx1-1])\n",
    "        first_idx = min(idx1,idx2)\n",
    "        second_idx = max(idx1,idx2)\n",
    "        temp = word.pop(first_idx)\n",
    "        word.insert(second_idx, temp)\n",
    "        return ''.join(map(str,word))\n",
    "        \n",
    "    def hash_tokenize(self, data):\n",
    "        tokenized_id = [subword_hashes(w) for w in data]\n",
    "        return tokenized_id\n",
    "    \n",
    "    def padding(self, data, padding_idx=0):\n",
    "        if len(data) >= max_sample_len:\n",
    "            return torch.tensor(data[:max_sample_len], dtype = torch.long).to(device)\n",
    "        data.extend(np.array([[padding_idx]*max_sw_hash_len]*(max_sample_len - len(data))))\n",
    "        return torch.tensor(data, dtype = torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afc79c2c-6adb-43d3-b40b-fd03f89fd89c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class YahooClassifier(Module):\n",
    "    def __init__(self, word_embedding, embedding_dim, class_zize,device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = word_embedding.to(device)\n",
    "        self.linear_classifier = Linear(embedding_dim, class_size).to(device)\n",
    "    \n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"\n",
    "        toekn_ids: (batch_size, worrd_num, hash_size)\n",
    "        \"\"\"\n",
    "#         print(token_ids.shape)\n",
    "        # (batch_size, words_num, embedding_dim)\n",
    "        outputs = self.word_embedding(token_ids).to(self.device)\n",
    "#         print(outputs.shape)\n",
    "        # (batch_size, embedding_dim)\n",
    "        outputs = torch.max(outputs, dim=1)[0].to(self.device)\n",
    "#         print(outputs.shape)\n",
    "        # (batch_size, class_size)\n",
    "        outputs = self.linear_classifier(outputs).to(self.device)\n",
    "#         print(outputs.shape)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1b0f877-1ee2-497b-9a8d-8d8b4b2b12a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 10000\n",
    "batch_size = 8\n",
    "emb_dim = 50\n",
    "num_emb = int(2e6+1)\n",
    "context_size = 3\n",
    "neg_num = 5\n",
    "uniform = True\n",
    "max_epochs = 100\n",
    "class_size = 10\n",
    "misspell_freq = 0.5\n",
    "\n",
    "if uniform: dist = 'uniform'\n",
    "else: dist = 'noisedist'\n",
    "if max_epochs is None:\n",
    "    epoch = 'ULepochs'\n",
    "else: epoch = f'{max_epochs}e'\n",
    "emb_path = f\"./SubwordEmbedding/trained_model/trained_model_{emb_dim}d_{dist}_{epoch}_{context_size}w\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5fbd72f-ada6-45b3-bc2c-77fbad31811f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 9000/9000 [00:07<00:00, 1193.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377\n",
      "995\n"
     ]
    }
   ],
   "source": [
    "dataset = YahooDataset(max_samples=max_samples, local_dir=\"small_yahoo_dataset\")\n",
    "subword_hashes = subwordhash(dataset.train)\n",
    "\n",
    "word_num = subword_hashes.word_num\n",
    "max_sw_hash_len = subword_hashes.max_hash\n",
    "max_sample_len = subword_hashes.max_sample\n",
    "print(max_sw_hash_len)\n",
    "print(max_sample_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a36d172a-0c12-4c95-93fb-855fb8c14b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: trained_model_50d_uniform_100e_3w\n"
     ]
    }
   ],
   "source": [
    "word_embedding = SubwordEmbedding(num_embeddings = num_emb, embedding_dim = emb_dim, device = device, padding_idx = 0)\n",
    "word_embedding = word_embedding.to(device)\n",
    "word_embedding.load_state_dict(torch.load(emb_path))\n",
    "print(f'Loaded model: trained_model_{emb_dim}d_{dist}_{epoch}_{context_size}w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d14e6f7-8816-4df1-b504-900530f8b9f5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessor = Hash_Preprocessor()\n",
    "\n",
    "dataset.train.set_preprocessor(preprocessor)\n",
    "dataset.val.set_preprocessor(preprocessor)\n",
    "dataset.test.set_preprocessor(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c44af8e-7216-4821-9091-59f7759fab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(dataset.train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset.val, batch_size=batch_size, shuffle=False)\n",
    "dataloader_test = DataLoader(dataset.test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fe9c9b1-b5e6-4721-a8c4-366853bedb8c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "misspell_dataset = YahooDataset(max_samples=max_samples, local_dir=\"small_yahoo_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e36ad0ae-9d55-4016-a52c-235e7e1c8de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Misspell_Preprocessor()\n",
    "\n",
    "misspell_dataset.train.set_preprocessor(preprocessor)\n",
    "misspell_dataset.val.set_preprocessor(preprocessor)\n",
    "misspell_dataset.test.set_preprocessor(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30d5b3ff-5515-432a-876e-a2c58e608212",
   "metadata": {},
   "outputs": [],
   "source": [
    "msploader_train = DataLoader(misspell_dataset.train, batch_size=batch_size, shuffle=True)\n",
    "msploader_val = DataLoader(misspell_dataset.val, batch_size=batch_size, shuffle=False)\n",
    "msploader_test = DataLoader(misspell_dataset.test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04171d94-15db-4cf4-b336-5c7ba04bf8c1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "yahoo_classifier = YahooClassifier(word_embedding, emb_dim, class_size,device).to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c5d92d3-efa8-4f12-aa96-868490ad612b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "for batch in dataloader_train:\n",
    "    outputs = yahoo_classifier(batch[\"input\"])\n",
    "    print(outputs.shape)\n",
    "    print(outputs)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1b2f732-c81a-43a3-82fa-b45ab7127324",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifier_model = MulticlassSentenceClassificationTrainerModule(yahoo_classifier).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9f38538-962f-4023-bd3f-02aa690d8926",
   "metadata": {},
   "outputs": [],
   "source": [
    "misspell = True\n",
    "if misspell:\n",
    "    logger = pl.loggers.CSVLogger(\"Classification/logs\", name = f\"MisspellText_{emb_dim}d\")\n",
    "else:\n",
    "    logger = pl.loggers.CSVLogger(\"Classification/logs\", name = f\"CorrectedText_{emb_dim}d\")\n",
    "    \n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath = \"Classification/checkpoints\",\n",
    "    filename = 'best_model',\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'min'\n",
    ")\n",
    "class LitProgressBar(pl.callbacks.ProgressBar):\n",
    "    def init_validation_tqdm(self):\n",
    "        bar = tqdm(disable=True)\n",
    "        return bar\n",
    "bar = LitProgressBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b26d37bf-211e-4a46-ade1-2e74856ed7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\device_parser.py:130: LightningDeprecationWarning: Parsing of the Trainer argument gpus='0' (string) will change in the future. In the current version of Lightning, this will select CUDA device with index 0, but from v1.5 it will select gpus [] (same as gpus=0 (int)).\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "mode = 'cor'\n",
    "loader = {\n",
    "    'msp':{'train':msploader_train, 'val':msploader_val, 'test':msploader_test},\n",
    "    'cor':{'train':dataloader_train, 'val':dataloader_val, 'test':dataloader_test}\n",
    "}\n",
    "torch.cuda.empty_cache()\n",
    "trainer = pl.Trainer(logger = logger, \n",
    "                     gpus = '0', \n",
    "                     callbacks=[checkpoint, bar], \n",
    "                     num_sanity_val_steps=0, \n",
    "                     max_epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "948329b7-11fa-4ae0-9d68-b75434db3a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\device_parser.py:130: LightningDeprecationWarning: Parsing of the Trainer argument gpus='0' (string) will change in the future. In the current version of Lightning, this will select CUDA device with index 0, but from v1.5 it will select gpus [] (same as gpus=0 (int)).\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | YahooClassifier | 100 M \n",
      "------------------------------------------\n",
      "100 M     Trainable params\n",
      "0         Non-trainable params\n",
      "100 M     Total params\n",
      "400.002   Total estimated model params size (MB)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7baee6854bb24bb2845562a357edf97c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:897: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn('Detected KeyboardInterrupt, attempting graceful shutdown...')\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(classifier_model, \n",
    "            train_dataloader = loader[mode]['train'],\n",
    "            val_dataloaders = loader[mode]['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2ee88dc-96a8-4d72-a3e7-789e6458888f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-a836c4bd2cc3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# w_e.load_state_dict(checkpoint['state_dict'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mclassifier_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'state_dict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'optimizer_state_dict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'epoch'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "checkpoint = torch.load(\"./Classification/checkpoints/best_model.ckpt\")\n",
    "# print(checkpoint)\n",
    "w_e = SubwordEmbedding(num_embeddings = num_emb, embedding_dim = emb_dim, device = device, padding_idx = 0)\n",
    "yahoo_classifier = YahooClassifier(word_embedding, emb_dim, class_size,device).to(device)\n",
    "classifier_model = MulticlassSentenceClassificationTrainerModule(yahoo_classifier).to(device)\n",
    "# w_e.load_state_dict(checkpoint['state_dict'])\n",
    "classifier_model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f143530-2956-4dd1-a054-b66c4a81a08d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MulticlassSentenceClassificationTrainerModule:\n\tWhile copying the parameter named \"model.word_embedding.subword_embedding.weight\", whose dimensions in the model are torch.Size([2000001, 50]) and whose dimensions in the checkpoint are torch.Size([2000001, 50]), an exception occurred : ('CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.',).\n\tWhile copying the parameter named \"model.linear_classifier.weight\", whose dimensions in the model are torch.Size([10, 50]) and whose dimensions in the checkpoint are torch.Size([10, 50]), an exception occurred : ('CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.',).\n\tWhile copying the parameter named \"model.linear_classifier.bias\", whose dimensions in the model are torch.Size([10]) and whose dimensions in the checkpoint are torch.Size([10]), an exception occurred : ('CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.',).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-cf8edbdda734>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloaders\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(self, model, test_dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmodel_provided\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 578\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtested_ckpt_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load_ckpt_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m         \u001b[1;31m# run test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m__load_ckpt_weights\u001b[1;34m(self, ckpt_path)\u001b[0m\n\u001b[0;32m   1151\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbarrier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1153\u001b[1;33m         self.training_type_plugin.restore_model_state_from_ckpt_path(\n\u001b[0m\u001b[0;32m   1154\u001b[0m             \u001b[0mckpt_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\plugins\\training_type\\training_type_plugin.py\u001b[0m in \u001b[0;36mrestore_model_state_from_ckpt_path\u001b[1;34m(self, ckpt_path, map_location)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;31m# hook: give user access to checkpoint if needed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_load_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'state_dict'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mckpt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1405\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1406\u001b[1;33m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[0;32m   1407\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0;32m   1408\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MulticlassSentenceClassificationTrainerModule:\n\tWhile copying the parameter named \"model.word_embedding.subword_embedding.weight\", whose dimensions in the model are torch.Size([2000001, 50]) and whose dimensions in the checkpoint are torch.Size([2000001, 50]), an exception occurred : ('CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.',).\n\tWhile copying the parameter named \"model.linear_classifier.weight\", whose dimensions in the model are torch.Size([10, 50]) and whose dimensions in the checkpoint are torch.Size([10, 50]), an exception occurred : ('CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.',).\n\tWhile copying the parameter named \"model.linear_classifier.bias\", whose dimensions in the model are torch.Size([10]) and whose dimensions in the checkpoint are torch.Size([10]), an exception occurred : ('CUDA error: device-side assert triggered\\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.',)."
     ]
    }
   ],
   "source": [
    "trainer.test(test_dataloaders = loader[mode]['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf59549-105e-4380-83eb-b9b46354125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(test_dataloaders = loader['msp']['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6959f5fa-040d-4923-9cc9-a904344a655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f\"./Classification/trained_model/trained_classification_model_{emb_dim}d_{epoch}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "04bcb7e5-1b7b-477e-a195-2623a3900adb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-72620888c925>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myahoo_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m                 \u001b[0m_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    494\u001b[0m         \u001b[1;31m# .cpu() on the underlying Storage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'cpu'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m             \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m         \u001b[1;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melement_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\storage.py\u001b[0m in \u001b[0;36mcpu\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;34m\"\"\"Returns a CPU copy of this storage if it's not already on the CPU\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdouble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36mtype\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    538\u001b[0m         \u001b[1;31m# or on typing_extensions module on Python >= 3.6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# type: ignore[attr-defined]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 540\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m     \u001b[0m__new__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_lazy_new\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_utils.py\u001b[0m in \u001b[0;36m_type\u001b[1;34m(self, dtype, non_blocking, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "torch.save(yahoo_classifier.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf297e6-0b55-42b2-a510-76b1682d1a62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "b8a566f2-f060-4abc-b33b-afaf9dbacb0e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def a(word):\n",
    "    idx1 = np.random.randint(len(word))\n",
    "    if idx1 == 0:\n",
    "        idx2 = idx1 + 1\n",
    "    elif idx1 == len(word)-1:\n",
    "        idx2 = idx1 -1\n",
    "    else:\n",
    "        idx2 = np.random.choice([idx1+1,idx1-1])\n",
    "    first_idx = min(idx1,idx2)\n",
    "    second_idx = max(idx1,idx2)\n",
    "    temp = word.pop(first_idx)\n",
    "    word.insert(second_idx, temp)\n",
    "    return ''.join(map(str,word))\n",
    "def b(w):\n",
    "    w[2] = 'b'\n",
    "func = {\n",
    "    1:a,\n",
    "    2:b\n",
    "}\n",
    "\n",
    "x = 'strength'\n",
    "y = func[1](list(x))\n",
    "y"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a7344240-152d-4b3c-b850-768db014f32d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def p(word):\n",
    "    word[0] = 10\n",
    "\n",
    "a = [1,2,3,4]\n",
    "p(a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d1820fc-2cba-4c30-b8dd-3da5b4f8966f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def misspell(data):\n",
    "    random_freq = False\n",
    "    if random_freq: \n",
    "        msp_f = np.random.uniform(0.1,0.5)\n",
    "    else: \n",
    "        msp_f = misspell_freq\n",
    "    misspell_num = int(len(data)*msp_f)\n",
    "    misspell_idx = np.random.choice(len(data), misspell_num, replace = False)\n",
    "    m_type = {i:mt for i,mt in enumerate(np.random.randint(0, 4, misspell_num))}\n",
    "    m_dict = {0:delete, 1:insert, 2:replace, 3:swap}\n",
    "    for i in range(misspell_num):\n",
    "        mp = data[misspell_idx[i]]\n",
    "        mp2 = mp[:]\n",
    "        if len(mp) > 1:\n",
    "            mp2 = m_dict[m_type[i]](list(mp))\n",
    "        else:\n",
    "            mp2 = replace(list(mp))\n",
    "        print(f'{mp} : {mp2}')\n",
    "\n",
    "def delete(word):\n",
    "    idx = np.random.randint(len(word))\n",
    "    word.pop(idx)\n",
    "    return ''.join(map(str,word))\n",
    "\n",
    "def insert(word):\n",
    "    idx = np.random.randint(len(word))\n",
    "    letter = np.random.choice(L)\n",
    "    word.insert(idx, letter)\n",
    "    return ''.join(map(str,word))\n",
    "\n",
    "def replace(word):\n",
    "    idx = np.random.randint(len(word))\n",
    "    letter = np.random.choice(L)\n",
    "    word.pop(idx)\n",
    "    word.insert(idx,letter)\n",
    "    return ''.join(map(str,word))\n",
    "\n",
    "def swap(word):\n",
    "    idx1 = np.random.randint(len(word))\n",
    "    if idx1 == 0:\n",
    "        idx2 = idx1 + 1\n",
    "    elif idx1 == len(word)-1:\n",
    "        idx2 = idx1 -1\n",
    "    else:\n",
    "        idx2 = np.random.choice([idx1+1,idx1-1])\n",
    "    first_idx = min(idx1,idx2)\n",
    "    second_idx = max(idx1,idx2)\n",
    "    temp = word.pop(first_idx)\n",
    "    word.insert(second_idx, temp)\n",
    "    return ''.join(map(str,word))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df4789e6-8af6-4124-b7d6-2c4c1b2e4420",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "sample = word_tokenize('Fuck your bitch-ass horny jail. Can a man get a fucking hard-on without being assaulted by a dumbass looking dog bitch holding a png ass baseball bat over the fucking internet? Can a male have a crumb or two of libido without being “bonked” and sent to “horny jail”? Can I have a fucking natural human instinct without being fucking beaten and imprisoned by this little bitch boy Cheems? “Oh goody this man is being an average person expressing his lust over his screen where he feels safe to share it? Luckily I have ‘Bonk, go to horny jail’ permanently saved to my fucking clipboard!” Fuck off damnit. Let a guy be horny. Let a guy be in heat. Let a guy feel sexual attraction to a female for fuck’s sake. Slap yourself across the face. Wake yourself up from the sad life you lead, preventing strangers on the internet from being happily horny behind their screens. Wake the fuck up from your “job” at r/bonkpatrol and make a fucking living for once you fucking anti-lust cretin. Go the fuck outside, maybe you can beat couples with baseball bats and get arrested out there. You’d serve a greater purpose to society in a cage than in your rgb room with a high-end pc to harass internet people with. Go fucking retard jail you piss goblin')\n",
    "# print(sample)\n",
    "misspell(sample)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d4e51907-0929-43e1-909e-baaf73190174",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "x = [1,2,30]\n",
    "\n",
    "a = x[2]\n",
    "a = 10\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7248b6ca-f1e1-4de4-8ce6-66602e82259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {True:1, False:2}\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03975f85-af2c-4e4d-a8fc-ac15901e6e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]\n",
    "b = [4,5,6]\n",
    "c = [7,8,9]\n",
    "ld = {\n",
    "    'msp':{'train':a},\n",
    "    'cor':{'train':c}\n",
    "}\n",
    "print(ld['msp']['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1d939d-c7c6-4c0c-932d-ccdc99c1fc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
