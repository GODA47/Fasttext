{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292b811e-c7dd-477b-908a-508b3bd4f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# from Utilities.NLP_ModelTrainers.SentenceClassification.MulticlassSentenceClassificationModule import MulticlassSentenceClassificationTrainerModule\n",
    "\n",
    "from nlp_datasets import YahooDataset\n",
    "from nlp_datasets import BaseDataset\n",
    "from nlp_datasets import WordSim353Dataset\n",
    "from nlp_modeltrainers import BaseTrainerModule\n",
    "from nlp_metrics import Metrics\n",
    "\n",
    "import torch\n",
    "import fastwer\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.nn import Module, Linear, Embedding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = 'cpu'\n",
    "\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a6cea0b-b136-4db3-ad7a-7cdc15bba56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class subwordhash:\n",
    "    def __init__(self, dataset):\n",
    "        word_num, hash_len, sample_len = self.average_subword_num(dataset)\n",
    "        self.word_num = word_num\n",
    "        self.max_hash = hash_len\n",
    "        self.max_sample = sample_len\n",
    "        \n",
    "    def __call__(self, word):\n",
    "        return self.subword_hashes(word, max_hash_num = self.max_hash)\n",
    "    \n",
    "    def fnv1a(self, txt, K = int(2e6 + 1)):\n",
    "        # 64 bit fnv-1a\n",
    "        txt = bytes(txt, 'utf-8')\n",
    "        hval = 0xcbf29ce484222325\n",
    "        fnv_prime = 0x100000001b3\n",
    "        for c in txt:\n",
    "            hval = hval ^ c\n",
    "            hval = (hval * fnv_prime) % K\n",
    "        return hval + 1        \n",
    "\n",
    "    def subword_hashes(self, word, max_hash_num = None, get_len = False):\n",
    "        sub_hash = []\n",
    "        tword = '<' + word + '>'\n",
    "        sub_hash.append(self.fnv1a(tword))\n",
    "        for n in range(3,7):\n",
    "            for i in range(len(tword)-n+1):\n",
    "                sub_hash.append(self.fnv1a(tword[i:i+n]))\n",
    "                if len(sub_hash) == max_hash_num:\n",
    "                    return np.array(sub_hash[:max_hash_num])\n",
    "        if max_hash_num is not None:\n",
    "            sub_hash.extend([0]*(max_hash_num - len(sub_hash)))\n",
    "        if get_len:\n",
    "            return len(sub_hash)\n",
    "        return np.array(sub_hash)\n",
    "\n",
    "    def average_subword_num(self, dataset):\n",
    "        max_sample_len = 0\n",
    "        hash_len_dist = {}\n",
    "        len_dist = {}\n",
    "        for sample in tqdm(dataset):\n",
    "            tokens = word_tokenize(sample[\"input\"])\n",
    "            if len(tokens) not in len_dist:\n",
    "                len_dist[len(tokens)] = 0\n",
    "            len_dist[len(tokens)] += 1\n",
    "            max_sample_len = max(max_sample_len, len(tokens))\n",
    "            \n",
    "        for L in list(len_dist):\n",
    "            hash_len_dist[self.subword_hashes('a'*L, get_len = True)] = len_dist[L]\n",
    "        \n",
    "        total = 0\n",
    "        weighted_hash_len = []\n",
    "        for L in list(hash_len_dist):\n",
    "            total += hash_len_dist[L]\n",
    "            weighted_hash_len.append(hash_len_dist[L]*L)\n",
    "        avg = sum(weighted_hash_len)/total\n",
    "        \n",
    "        return int(total), int(avg), max_sample_len\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1b0f877-1ee2-497b-9a8d-8d8b4b2b12a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_samples = 10000\n",
    "batch_size = 8\n",
    "emb_dim = 50\n",
    "num_emb = int(2e6+1)\n",
    "context_size = 3\n",
    "neg_num = 5\n",
    "uniform = True\n",
    "max_epochs = 100\n",
    "classify_epochs = 50\n",
    "class_size = 10\n",
    "misspell_freq = 0.5\n",
    "\n",
    "if uniform: dist = 'uniform'\n",
    "else: dist = 'noisedist'\n",
    "epoch = f'{max_epochs}e'\n",
    "emb_path = f\"./SubwordEmbedding/trained_model/trained_model_{emb_dim}d_{dist}_{epoch}_{context_size}w\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e3833b-9e78-4339-9d45-1a8a2d09939e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open(\"../NLP_Utilities/NLP_Datasets/datasets/spelling_similarity_corpus/words_corpus.txt\", \"r\") as f:\n",
    "#     words_corpus = [line.split(\":\") for line in f.read().split(\"\\n\")]\n",
    "    \n",
    "# with open(\"../NLP_Utilities/NLP_Datasets/datasets/spelling_similarity_corpus/anagram_corpus.txt\", \"r\") as f:\n",
    "#     anagram_corpus = [line.split(\":\") for line in f.read().split(\"\\n\")]\n",
    "    \n",
    "# with open(\"../NLP_Utilities/NLP_Datasets/datasets/spelling_similarity_corpus/misspellings_corpus.txt\", \"r\") as f:\n",
    "#     misspellings_corpus = [line.split(\":\") for line in f.read().split(\"\\n\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "188ad52a-0d92-4e58-87ca-fc8e921b7a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubwordEmbedding(Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, device, padding_idx = 0):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.device =  device\n",
    "        self.padding_idx = padding_idx\n",
    "        \n",
    "        self.subword_embedding = Embedding(num_embeddings = num_embeddings, \n",
    "                                           embedding_dim = embedding_dim, \n",
    "                                           padding_idx = padding_idx)\n",
    "    def forward(self, token_ids):\n",
    "        # token_ids: (batch_size, word_num, hash_size)\n",
    "        # return: (batch_size, word_num, embedding_dim)\n",
    "        debug = False\n",
    "        \n",
    "        subword_embed = self.subword_embedding(token_ids)\n",
    "        # (batch_size, word_num, hash_size, embedding_dim)\n",
    "        if debug: print(\"subword_embed.shape: \", subword_embed.shape)\n",
    "        \n",
    "        word_embed = subword_embed.sum(dim = len(subword_embed.shape) -2).to(self.device)\n",
    "        # (batch_size, word_num, embedding_dim)\n",
    "        if debug: print(\"word_embed.shape: \", word_embed.shape)\n",
    "        \n",
    "        if debug: print(\"\\n########################################\\n\")\n",
    "        return word_embed\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa956606-66ae-4634-ac2d-e3eb117811e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def similarity_eval(word_corpus, word_embedding):\n",
    "#     mode = \"Cosine_Similarity\"\n",
    "    spearman_corr = Metrics(\"Spearman\")\n",
    "    cer_func = Metrics(\"CER\")\n",
    "    mae = []\n",
    "    less = []\n",
    "    more = []\n",
    "    similarities = []\n",
    "    target_similarities = []\n",
    "    if isinstance(word_corpus[0], dict):\n",
    "        word1 = list(word_corpus[0].keys())[0]\n",
    "        word2 = list(word_corpus[0].keys())[1]\n",
    "    else:\n",
    "        word1 = 0\n",
    "        word2 = 1\n",
    "        \n",
    "    for w in tqdm(word_corpus):\n",
    "        w1 = subword_hashes(w[word1])\n",
    "        w2 = subword_hashes(w[word2])\n",
    "        subword_embed = word_embedding.subword_embedding(torch.tensor([w1,w2],dtype = torch.long)).to(device)\n",
    "        word_embed = subword_embed.sum(dim = len(subword_embed.shape) -2).unsqueeze(1).to(device)\n",
    "#         print(word_embed[0].shape, word_embed[1].shape)\n",
    "        print(Metrics(mode)(word_embed[0], word_embed[1]))\n",
    "        model_sim = float(spearman_co(word_embed[0], word_embed[1])[mode])\n",
    "        similarities.append(model_sim)\n",
    "        target_sim = 1 - cer_func(w[word1], w[word2])[\"CER\"]/100\n",
    "        target_similarities.append(target_sim)\n",
    "        mae.append(abs(target_sim - model_sim))\n",
    "        if target_sim - model_sim >=0:\n",
    "            less.append(abs(target_sim - model_sim))\n",
    "        elif target_sim - model_sim < 0:\n",
    "            more.append(abs(target_sim - model_sim))\n",
    "            \n",
    "    print(f\"MAE: {np.mean(mae)}\")\n",
    "    print(f\"MAE (More): {np.mean(more)}\")\n",
    "    print(f\"MAE (Less): {np.mean(less)}\")\n",
    "    plt.figure(figsize=[20,5])\n",
    "    plt.plot(similarities)\n",
    "    plt.plot(target_similarities)\n",
    "    plt.xticks(range(0,20), labels=range(1,21))\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def similarity_plot(word_embedding, max_char_len = 20):\n",
    "    np.random.seed(0)\n",
    "    words_num = 1000\n",
    "    chars = list('qwertyuiopasdfghjklzxcvbnm')\n",
    "    similarities = []\n",
    "    target_similarities = []\n",
    "    for char_len in tqdm(range(1,max_char_len +1)):\n",
    "        g_words1 = [\"\".join(sampling_chars) for sampling_chars in np.random.choice(chars, size=[words_num, char_len], replace=True)]\n",
    "        g_words2 = [\"\".join(sampling_chars) for sampling_chars in np.random.choice(chars, size=[words_num, char_len], replace=True)]\n",
    "        \n",
    "        gw1 = [subword_hashes(w) for w in g_words1]\n",
    "        gw2 = [subword_hashes(w) for w in g_words2]\n",
    "        subword_embed = word_embedding.subword_embedding(torch.tensor([gw1,gw2],dtype = torch.long)).to(device)\n",
    "        g_word_embed = subword_embed.sum(dim = len(subword_embed.shape) -2).to(device)\n",
    "#         print(g_word_embed.shape)\n",
    "        model_sim = F.cosine_similarity(g_word_embed[0], g_word_embed[1]).to(torch.device(\"cpu\")).detach().numpy()\n",
    "#         print(model_sim.shape)\n",
    "        similarities.append(model_sim.mean())\n",
    "        target_sim = 1 - fastwer.score(g_words1, g_words2, char_level = True)/100\n",
    "#         print(target_sim)\n",
    "        target_similarities.append(target_sim)\n",
    "    \n",
    "    plt.figure(figsize=[20,5])\n",
    "    plt.plot(similarities)\n",
    "    plt.plot(target_similarities)\n",
    "    plt.xticks(range(0,20), labels=range(1,21))\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a402731-f088-44e7-baa5-2d2a825bea49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: trained_model_50d_100e\n",
      "2000001\n"
     ]
    }
   ],
   "source": [
    "word_embedding = SubwordEmbedding(num_embeddings = num_emb, embedding_dim = emb_dim, device = device, padding_idx = 0)\n",
    "word_embedding.load_state_dict(torch.load(emb_path))\n",
    "print(f'Loaded model: trained_model_{emb_dim}d_{epoch}')\n",
    "print(word_embedding.num_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cb72d3a-c159-4420-b2bd-5e5c67ce047b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 9000/9000 [00:06<00:00, 1378.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377\n",
      "995\n"
     ]
    }
   ],
   "source": [
    "dataset = YahooDataset(max_samples=max_samples, local_dir=\"small_yahoo_dataset\")\n",
    "subword_hashes = subwordhash(dataset.train)\n",
    "\n",
    "\n",
    "max_sw_hash_len = subword_hashes.max_hash\n",
    "max_sample_len = subword_hashes.max_sample\n",
    "print(max_sw_hash_len)\n",
    "print(max_sample_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3509a4e-a08f-4e53-8791-62ca02a03367",
   "metadata": {},
   "outputs": [],
   "source": [
    "WS353ds = WordSim353Dataset(max_samples = max_samples, local_dir = \"WordSim353Dataset\")\n",
    "misspellings_corpus = WS353ds.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86dd4b07-10fa-44d2-ae0b-c12297b55e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word1': 'ministry', 'word2': 'culture', 'similarity': 4.69}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misspellings_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac7c0b9b-a8dc-4f5b-a0f2-3f7449cf6c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                | 0/35 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Spearman'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-b0efcf6da463>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msimilarity_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmisspellings_corpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-58c8fa274846>\u001b[0m in \u001b[0;36msimilarity_eval\u001b[1;34m(word_corpus, word_embedding)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mword_embed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubword_embed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubword_embed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#         print(word_embed[0].shape, word_embed[1].shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_embed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_embed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mmodel_sim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMetrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_embed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_embed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0msimilarities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_sim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nlp_metrics\\Metrics.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, metrics, names, **metrics_arguments)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname2metric\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m                        \u001b[1;33m{\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname2metric\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_arguments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics_arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nlp_metrics\\Metrics.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname2metric\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m                        \u001b[1;33m{\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname2metric\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_arguments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics_arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Spearman'"
     ]
    }
   ],
   "source": [
    "similarity_eval(misspellings_corpus, word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab1bfb8-be21-4952-b8d3-4cdccd3dfebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Metrics('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83940be-1c6e-4bdb-8b36-35674a5442a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "misspellings_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd34fb02-eb5a-4922-9e76-fd468053ddc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad50ef-59a7-4b71-ae17-970bb5575f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 = torch.tensor([[1,1,1,1],[2,2,2,2],[3,3,3,3]], dtype = torch.long)\n",
    "# t2 = torch.rand()\n",
    "# out = F.cosine_similarity(t1,t2)\n",
    "# print(out.shape)\n",
    "# print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5eb0ea-7078-4ce7-8ecc-2cfcdc48322b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa6fd1-7a54-47bb-93b9-0a02584cceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastwer\n",
    "hypo = ['transporting']\n",
    "ref = ['black']\n",
    "\n",
    "a = fastwer.score_sent(hypo[0], ref[0], char_level=True)\n",
    "print(1-a/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6959154c-e115-4dc5-8f75-863b37f9a547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
