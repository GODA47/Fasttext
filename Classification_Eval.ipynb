{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c757538e-a5d2-456b-8eea-a4e30b8a8e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from nlp_datasets import YahooDataset\n",
    "from nlp_datasets import BaseDataset\n",
    "from nlp_modeltrainers import BaseTrainerModule\n",
    "from nlp_modeltrainers.sentence_classification import MulticlassSentenceClassificationTrainerModule\n",
    "# from nlp_modeltrainers import VectorCosineSimilarityTrainerModule\n",
    "\n",
    "\n",
    "import torch\n",
    "import fastwer\n",
    "from string import ascii_letters as letters\n",
    "L = list(letters)\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.nn import Module, Linear, Embedding\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "dev = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "\n",
    "device = torch.device(dev)\n",
    "CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d11cc15-252d-4da8-b320-509e97bce15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.SubwordHash import subwordhash\n",
    "from utils.SubwordEmbedding import subwordembedding\n",
    "from utils.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc292d2-caa9-4908-8e5a-8c704ceb7430",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Hash_Preprocessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        tokenized = word_tokenize(sample[\"input\"])\n",
    "        tokenized_hashes = self.hash_tokenize(tokenized)\n",
    "        output_id = self.padding(tokenized_hashes, padding_idx=0)\n",
    "        \n",
    "        return {\"input\": output_id, \"target\": sample['target']-1}\n",
    "    \n",
    "    def hash_tokenize(self, data):\n",
    "        tokenized_id = [subword_hashes(w) for w in data]\n",
    "        return tokenized_id\n",
    "    \n",
    "    def padding(self, data, padding_idx=0):\n",
    "        if len(data) >= max_sample_len:\n",
    "            return torch.tensor(data[:max_sample_len], dtype = torch.long).to(device)\n",
    "        data.extend(np.array([[padding_idx]*max_sw_hash_len]*(max_sample_len - len(data))))\n",
    "        return torch.tensor(data, dtype = torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8032c0c6-2ba7-4047-9255-0f1dd2e2abb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Misspell_Preprocessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        random_freq = True\n",
    "        tokenized = {i:w for i,w in enumerate(word_tokenize(sample[\"input\"]))}\n",
    "        tokenized = self.misspell(tokenized, random_freq)\n",
    "        tokenized_hashes = self.hash_tokenize(tokenized)\n",
    "        output_id = self.padding(tokenized_hashes, padding_idx=0)\n",
    "        \n",
    "        return {\"input\": output_id, \"target\": sample['target']-1}\n",
    "    \n",
    "    def misspell(self, data, random_freq=False):\n",
    "        if random_freq: \n",
    "            msp_f = np.random.uniform(0.1,0.5)\n",
    "        else: \n",
    "            msp_f = misspell_freq\n",
    "        misspell_num = int(len(data)*msp_f)\n",
    "        misspell_idx = np.random.choice(len(data), misspell_num, replace = False)\n",
    "        m_type = {i:mt for i,mt in enumerate(np.random.randint(0, 4, misspell_num))}\n",
    "        m_dict = {0:self.delete, 1:self.insert, 2:self.replace, 3:self.swap}\n",
    "        for i in range(misspell_num):\n",
    "            mp = data[misspell_idx[i]]\n",
    "            if len(mp) > 1:\n",
    "                mp = m_dict[m_type[i]](list(mp))\n",
    "            else:\n",
    "                mp = self.replace(list(mp))\n",
    "            data[misspell_idx[i]] = mp\n",
    "        return [data[w] for w in sorted(data)]\n",
    "    \n",
    "    def delete(self, word):\n",
    "        idx = np.random.randint(len(word))\n",
    "        word.pop(idx)\n",
    "        return ''.join(map(str,word))\n",
    "\n",
    "    def insert(self, word):\n",
    "        idx = np.random.randint(len(word))\n",
    "        letter = np.random.choice(L)\n",
    "        word.insert(idx, letter)\n",
    "        return ''.join(map(str,word))\n",
    "\n",
    "    def replace(self, word):\n",
    "        idx = np.random.randint(len(word))\n",
    "        letter = np.random.choice(L)\n",
    "        word.pop(idx)\n",
    "        word.insert(idx,letter)\n",
    "        return ''.join(map(str,word))\n",
    "\n",
    "    def swap(self, word):\n",
    "        idx1 = np.random.randint(len(word))\n",
    "        if idx1 == 0:\n",
    "            idx2 = idx1 + 1\n",
    "        elif idx1 == len(word)-1:\n",
    "            idx2 = idx1 -1\n",
    "        else:\n",
    "            idx2 = np.random.choice([idx1+1,idx1-1])\n",
    "        first_idx = min(idx1,idx2)\n",
    "        second_idx = max(idx1,idx2)\n",
    "        temp = word.pop(first_idx)\n",
    "        word.insert(second_idx, temp)\n",
    "        return ''.join(map(str,word))\n",
    "        \n",
    "    def hash_tokenize(self, data):\n",
    "        tokenized_id = [subword_hashes(w) for w in data]\n",
    "        return tokenized_id\n",
    "    \n",
    "    def padding(self, data, padding_idx=0):\n",
    "        if len(data) >= max_sample_len:\n",
    "            return torch.tensor(data[:max_sample_len], dtype = torch.long).to(device)\n",
    "        data.extend(np.array([[padding_idx]*max_sw_hash_len]*(max_sample_len - len(data))))\n",
    "        return torch.tensor(data, dtype = torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e83eea29-1d70-4368-bbea-1ca8379adf2e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 9000/9000 [00:08<00:00, 1079.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377\n",
      "995\n"
     ]
    }
   ],
   "source": [
    "dataset = YahooDataset(max_samples=max_samples, local_dir=\"../datasets/small_yahoo_dataset\")\n",
    "subword_hashes = subwordhash(dataset.train)\n",
    "\n",
    "word_num = subword_hashes.word_num\n",
    "max_sw_hash_len = subword_hashes.max_hash\n",
    "max_sample_len = subword_hashes.max_sample\n",
    "print(max_sw_hash_len)\n",
    "print(max_sample_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bad9af30-45b3-47b5-adfa-f564ccfc81f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: ../Trained_Models/SubwordEmbedding/trained_model/trained_model_50d_uniform_100e_3w\n"
     ]
    }
   ],
   "source": [
    "word_embedding = subwordembedding(num_embeddings = num_emb, embedding_dim = emb_dim, device = device, padding_idx = 0)\n",
    "word_embedding = word_embedding.to(device)\n",
    "word_embedding.load_state_dict(torch.load(emb_path))\n",
    "print(f'Loaded model: {emb_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45f454c2-8aa0-41b1-9495-ab55979d35ea",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessor = Hash_Preprocessor()\n",
    "dataset.train.set_preprocessor(preprocessor)\n",
    "dataset.val.set_preprocessor(preprocessor)\n",
    "dataset.test.set_preprocessor(preprocessor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5a0dad2-3f14-460c-a919-9a9efe06ee0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(dataset.train, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset.val, batch_size=batch_size, shuffle=False)\n",
    "dataloader_test = DataLoader(dataset.test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40b44f9a-c0c4-495a-ae48-1f9dfa0b03cc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "misspell_dataset = YahooDataset(max_samples=max_samples, local_dir=\"../datasets/misspell_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0e56e1d-487d-43aa-93db-dfb210db8c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Misspell_Preprocessor()\n",
    "\n",
    "misspell_dataset.train.set_preprocessor(preprocessor)\n",
    "misspell_dataset.val.set_preprocessor(preprocessor)\n",
    "misspell_dataset.test.set_preprocessor(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0673f1fc-e9c3-41ce-8422-d0260e064a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "msploader_train = DataLoader(misspell_dataset.train, batch_size=batch_size, shuffle=True)\n",
    "msploader_val = DataLoader(misspell_dataset.val, batch_size=batch_size, shuffle=False)\n",
    "msploader_test = DataLoader(misspell_dataset.test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fde1c29-d058-49e5-b77f-3f600536a961",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class YahooClassifier(Module):\n",
    "    def __init__(self, word_embedding, embedding_dim, class_zize,device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = word_embedding.to(device)\n",
    "        self.linear_classifier = Linear(embedding_dim, class_size).to(device)\n",
    "    \n",
    "    def forward(self, token_ids):\n",
    "        \"\"\"\n",
    "        toekn_ids: (batch_size, worrd_num, hash_size)\n",
    "        \"\"\"\n",
    "#         print(token_ids.shape)\n",
    "        # (batch_size, words_num, embedding_dim)\n",
    "        outputs = self.word_embedding(token_ids).to(self.device)\n",
    "#         print(outputs.shape)\n",
    "        # (batch_size, embedding_dim)\n",
    "#         print(outputs.shape)\n",
    "#         print(len(outputs.shape))\n",
    "        outputs = torch.max(outputs, dim=len(outputs.shape)-2)[0].to(self.device)\n",
    "#         print(outputs.shape)\n",
    "#         print(outputs)\n",
    "        # (batch_size, class_size)\n",
    "        outputs = self.linear_classifier(outputs).to(self.device)\n",
    "#         print(outputs.shape)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "987dd08e-45e5-413b-a83d-e748c337f848",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# worddata = YahooDataset(max_samples=max_samples, local_dir=\"../datasets/small_yahoo_dataset\")\n",
    "\n",
    "# data = []\n",
    "# for i in range(8960, 8976):\n",
    "#     data.append(dataset.test[i])\n",
    "# t_loader = DataLoader(data, batch_size = 1,shuffle = False)\n",
    "# for batch in dataloader_test:\n",
    "#     print(batch['input'].shape)\n",
    "#     break\n",
    "# # i = 8960\n",
    "# # for batch in tqdm(t_loader):\n",
    "# #     print(i, end = \" \")\n",
    "# #     print(batch['input'].shape)\n",
    "# #     out = yahoo_classifier.forward(batch['input'])\n",
    "# #     i += 1\n",
    "# print(len(worddata.test[8975]['input']))\n",
    "# print (dataset.test[8975])\n",
    "# print(yahoo_classifier.forward(dataset.test[8975]['input'].unsqueeze(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a61dda9a-ca42-4591-838f-ce15f1c49410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = f\"../Trained_Models/Classification/trained_model/trained_classification_model_{emb_dim}d_{classify_epochs}e_sumpool\"\n",
    "yahoo_classifier = YahooClassifier(word_embedding, emb_dim, class_size,device).to(device)\n",
    "yahoo_classifier.load_state_dict(torch.load(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12f76823-470e-4df8-a170-63d736931def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path_latesum = f\"../Trained_Models/Classification/trained_model/trained_classification_model_{emb_dim}d_{classify_epochs}e_latesum\"\n",
    "latesum_yahoo_classifier = YahooClassifier(word_embedding, emb_dim, class_size,device).to(device)\n",
    "latesum_yahoo_classifier.load_state_dict(torch.load(save_path_latesum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55360995-925e-4af8-9327-45be4331b01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_eval(models, test_loader):    \n",
    "    target_class = np.array([])\n",
    "    prediction = [np.array([])]*len(models)\n",
    "    first_model = True\n",
    "    for batch in tqdm(test_loader):\n",
    "        model = 0\n",
    "        for i in range(len(models)):\n",
    "            out = models[i].forward(batch['input'])\n",
    "            model_class = torch.max(out, dim = 1)[1].cpu().detach().numpy()\n",
    "            prediction[i] = np.append(prediction[i], model_class, 0)\n",
    "        tgc = batch['target'].cpu().detach().numpy()\n",
    "        target_class = np.append(target_class, tgc, 0)\n",
    "#     plt.figure(figsize=[20,5])\n",
    "#     for i in range(len(models)):\n",
    "#         plt.plot(miss[i])\n",
    "#     plt.grid()\n",
    "#     plt.show()\n",
    "    return prediction, target_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cdead30-6326-4441-b20b-7ce38e642eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1250/1250 [07:07<00:00,  2.92it/s]\n"
     ]
    }
   ],
   "source": [
    "prediction, target_class= classification_eval([yahoo_classifier], dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c715b34-b1bc-4efb-aa74-827722549c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1125/1125 [07:03<00:00,  2.66it/s]\n"
     ]
    }
   ],
   "source": [
    "prediction, target_class= classification_eval([yahoo_classifier], dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c7d4e6b-92bf-459e-bad0-4fa7c836d1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    4    0    0    0   28    0  501    1]\n",
      " [   0    0   11    0    0    0   43    0  958    1]\n",
      " [   0    0   12    0    0    0   26    0  647    1]\n",
      " [   0    0   11    0    0    0   57    0  751    2]\n",
      " [   0    0   16    0    0    0   41    0 1197    0]\n",
      " [   0    0    3    0    0    0   44    0  586    0]\n",
      " [   0    0   34    0    0    0  159    0 2085    1]\n",
      " [   0    0    9    0    0    0   53    0  572    1]\n",
      " [   0    0   12    0    0    0   19    0  467    0]\n",
      " [   0    0    4    0    0    0   28    0  613    2]]\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-9082991bb98a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# l = confusion_matrix(target_class,prediction[1])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'l' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "k = confusion_matrix(target_class,prediction[0])\n",
    "print(k)\n",
    "print()\n",
    "# l = confusion_matrix(target_class,prediction[1])\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f79d1b3-e809-4dba-86ac-736765828c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_res = [{{'correct':0, 'incorrect':0}}]*2\n",
    "print(len(target_class))\n",
    "for i in tqdm(range(len(target_class))):\n",
    "    for model in range(2):\n",
    "        if target_class[i] == prediction[model][i]:\n",
    "            class_res[model][int(target_class[i])]['correct'] += 1\n",
    "        else:\n",
    "            class_res[model][int(target_class[i])]['incorrect'] += 1\n",
    "print(class_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d5623-2383-4107-a359-bf6e8f006ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(2):\n",
    "    print(f'model #{j}')\n",
    "    for i,res in enumerate(class_res[j]):\n",
    "        print(res)\n",
    "#         print(f'class {i}: {res}\\t acc: {(res['correct'])*100/(res['correct']+res['incorrect'])}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d233ac94-7801-4ea4-b7a0-26de33509a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "for i,model in enumerate(miss):\n",
    "    total = len(model)\n",
    "    correct = collections.Counter(model)[0]\n",
    "    print(f'Accuracy #{i}: {correct*100/total} ({correct}/{total})')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c408cdb2-c92a-4650-aae1-4c67cc90f474",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_train = np.array([])\n",
    "\n",
    "\n",
    "for batch in tqdm(dataloader_train):\n",
    "    dist_train = np.append(dist_train, batch['target'],0)\n",
    "\n",
    "\n",
    "# classes_val = []\n",
    "# classes_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c20530-e2d2-4432-b66e-67dcd01cc457",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dist_train, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc41291-f7db-4b4b-bdf9-c5bab035b9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_val = np.array([])\n",
    "for batch in tqdm(dataloader_val):\n",
    "    dist_val = np.append(dist_val, batch['target'],0)\n",
    "plt.hist(dist_val, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e934f3a-4ca0-4320-b35b-df18f765026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_test = np.array([])\n",
    "for batch in tqdm(dataloader_test):\n",
    "    dist_test = np.append(dist_test, batch['target'],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9861ad45-4c5d-4298-8b8f-dc3c516f41f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dist_test, bins=10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ebe3d99-8ef5-45fd-8580-23d4618289fd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "test = dataset.train[1]['input']\n",
    "model = np.array([])\n",
    "target = np.array([])\n",
    "miss = np.array([])\n",
    "\n",
    "for batch in tqdm(dataloader_test):\n",
    "#     print(batch['input'].shape)\n",
    "    out = yahoo_classifier.forward(batch['input'])\n",
    "    model_classification = torch.max(out, dim = 1)[1].cpu().detach().numpy()\n",
    "    model = np.append(model, model_classification, 0)\n",
    "    target_classification = batch['target'].cpu().detach().numpy()\n",
    "    miss = np.append(miss, target_classification - model_classification)\n",
    "plt.figure(figsize=[20,5])\n",
    "plt.plot(miss)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "# output = yahoo_classifier.forward(test.unsqueeze)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "954477ec-5630-4c8c-85b5-2fd2b4df7628",
   "metadata": {
    "tags": []
   },
   "source": [
    "testload = DataLoader(dataset.test, batch_size=20, shuffle=False)\n",
    "for t in testload:\n",
    "    print(t['target'])\n",
    "    if not all(i<10 for i in t['target']):\n",
    "        print(\"################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24bc9d4-28cd-453d-bdee-0d610929dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3])\n",
    "b = np.array([4,5,6])\n",
    "c = np.append(a,b,0)\n",
    "c"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5895954b-3a36-472f-9c4a-6e2b256172d0",
   "metadata": {},
   "source": [
    "Society & Culture\n",
    "Science & Mathematics\n",
    "Health\n",
    "Education & Reference\n",
    "Computers & Internet\n",
    "Sports\n",
    "Business & Finance\n",
    "Entertainment & Music\n",
    "Family & Relationships\n",
    "Politics & Government"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
