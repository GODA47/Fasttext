{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "696af1c4-3c51-4fc5-a17d-0b2aa7621f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from nlp_datasets import YahooDataset\n",
    "from nlp_datasets import BaseDataset\n",
    "from nlp_datasets import WordSim353Dataset\n",
    "from nlp_modeltrainers import BaseTrainerModule\n",
    "from nlp_metrics import Metrics\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.nn import Module, Linear, Embedding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "import fastwer\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "dev = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ab17f94-5535-4d5d-b9ed-1cdd8556e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.SubwordHash import subwordhash\n",
    "from utils.SubwordEmbedding import subwordembedding\n",
    "from utils.config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fad6d11-6402-4954-91c8-264111190773",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Word_Preprocessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        tokenized = word_tokenize(sample[\"input\"])\n",
    "        return [w for w in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "492c2a14-ae8f-48db-8c6f-a46a44059b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hash_Preprocessor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        tokenized = word_tokenize(sample[\"input\"])\n",
    "        tokenized_hashes = self.hash_tokenize(tokenized)\n",
    "        output_id = self.padding(tokenized_hashes, padding_idx=0)\n",
    "        \n",
    "        return {\"input\": output_id, \"target\": sample['target']-1}\n",
    "    \n",
    "    def hash_tokenize(self, data):\n",
    "        tokenized_id = [subword_hashes(w) for w in data]\n",
    "        return tokenized_id\n",
    "    \n",
    "    def padding(self, data, padding_idx=0):\n",
    "        if len(data) >= max_sample_len:\n",
    "            return torch.tensor(data[:max_sample_len], dtype = torch.long).to(device)\n",
    "        data.extend(np.array([[padding_idx]*max_sw_hash_len]*(max_sample_len - len(data))))\n",
    "        return torch.tensor(data, dtype = torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71556256-ec1b-41a0-a298-992e0bde8386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FastTextDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 word_dataset, \n",
    "                 context_size,\n",
    "                 neg_num,\n",
    "                 device,\n",
    "                 uniform = False):\n",
    "        self.word_dataset = word_dataset\n",
    "        self.device = device\n",
    "        w_dict, w_to_id, v_id_f  = self.make_dict()\n",
    "        self.word_dict = w_dict\n",
    "        self.vocab_idx_freq = v_id_f\n",
    "        self.word_to_idx = w_to_id\n",
    "        self.context_size = context_size\n",
    "        self.neg_num = neg_num\n",
    "        self.data = np.array(self.get_training_data(context_size, neg_num, uniform), dtype = object)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        target = torch.from_numpy(subword_hashes(self.word_dict[int(self.data[idx,0])])).to(self.device)\n",
    "        context = torch.from_numpy(subword_hashes(self.word_dict[int(self.data[idx,1])])).to(self.device)\n",
    "        negatives = torch.from_numpy(np.array([subword_hashes(self.word_dict[n_idx]) for n_idx in self.data[idx,2]])).to(self.device)\n",
    "            \n",
    "        output = {'input':{ 'target':target, \n",
    "                            'context':context, \n",
    "                            'negatives':negatives},\n",
    "                  'target':[]}\n",
    "        return output\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def make_dict(self):\n",
    "        word_dict = {}\n",
    "        word_to_idx = {}\n",
    "        vocab_freq = {}\n",
    "        count = 0\n",
    "        for sample in self.word_dataset:\n",
    "            for word in sample:\n",
    "                word_dict[count] = word\n",
    "                if word not in vocab_freq:\n",
    "                    vocab_freq[word] = 0\n",
    "                    word_to_idx[word] = len(vocab_freq)-1\n",
    "                vocab_freq[word] += 1\n",
    "                count += 1\n",
    "        vocab_idx_freq = {word_to_idx[w]:vocab_freq[w] for w in vocab_freq}\n",
    "        return word_dict, word_to_idx, vocab_idx_freq\n",
    "    \n",
    "    def get_training_data(self, context_size, neg_num, uniform):\n",
    "        t_pos = 0\n",
    "        training_data = []\n",
    "        for sample in tqdm(self.word_dataset):\n",
    "            for tp in range(len(sample)):\n",
    "                context_pos = []\n",
    "                for sign in [-1,1]:\n",
    "                    for window in range(1, context_size+1):\n",
    "                        c_pos = t_pos + sign*(window)\n",
    "                        if c_pos not in range(len(sample)):\n",
    "                            break\n",
    "                        context_pos.append(c_pos)\n",
    "                \n",
    "                vocab_list, vocab_dist = self.negative_dist(t_pos, context_pos, uniform)\n",
    "                vocab = list(vocab_list)\n",
    "                for c_pos in context_pos:\n",
    "                    negative_idxs = np.random.choice(vocab_list, neg_num, p = vocab_dist, replace = True)\n",
    "                    training_data.append([t_pos, c_pos, negative_idxs])\n",
    "                t_pos += 1\n",
    "        return training_data\n",
    "    \n",
    "    def negative_dist(self, t_pos, c_pos, uniform):\n",
    "        vocab_idx_bag = self.vocab_idx_freq.copy()\n",
    "        exclude_words = []\n",
    "        for pos in c_pos:\n",
    "            exclude_words.append(self.word_to_idx[self.word_dict[pos]])\n",
    "        exclude_words.append(self.word_to_idx[self.word_dict[t_pos]])\n",
    "        exclude_words = set(exclude_words)\n",
    "        for w_idx in exclude_words:\n",
    "            vocab_idx_bag.pop(w_idx)\n",
    "            \n",
    "        if uniform:\n",
    "            noise_dist = np.array([1/len(vocab_idx_bag)]*len(vocab_idx_bag))\n",
    "        else:\n",
    "            word_freq = np.array(list(vocab_idx_bag))\n",
    "            unigram_dist = word_freq/np.sum(word_freq)\n",
    "            noise_dist = unigram_dist**(0.75)/np.sum(unigram_dist**(0.75))\n",
    "            \n",
    "        return list(vocab_idx_bag), noise_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48f22132-c9b0-4dbe-b605-e7352021c626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FastTextTrainer(BaseTrainerModule):\n",
    "    def __init__(self, word_embedding, device, debug = False):\n",
    "        super().__init__()\n",
    "        self.word_embedding = word_embedding\n",
    "        self._device = device\n",
    "        self.debug = debug\n",
    "        \n",
    "    def forward(self, target, context, negatives):\n",
    "        # (batch_size, ngram_size, embedding_dim)\n",
    "        target_vec = self.word_embedding(target)\n",
    "        # (batch_size, ngram_size, embedding_dim)\n",
    "        context_vec = self.word_embedding(context)\n",
    "        # (batch_size, negatives_num, ngram_size, embedding_dim)\n",
    "        negatives_vec = self.word_embedding(negatives)\n",
    "        \n",
    "        return target_vec, context_vec, negatives_vec\n",
    "    \n",
    "    def entropy_loss_func(self, vec1, vec2):\n",
    "        \"\"\"\n",
    "        vec1: (batch_size, ngram_size, embedding_dim)\n",
    "        vec2: (batch_size, ngram_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        if self.debug: \n",
    "            print('vec1.shape:\\t', vec1.shape)\n",
    "            print('vec2.shape:\\t', vec2.shape)\n",
    "            \n",
    "        vec_product = torch.mul(vec1, vec2).to(self._device)\n",
    "        if self.debug: print('vec_product.shape:\\t', vec_product.shape)\n",
    "        \"\"\" (batch_size, ngram_size, emb_dim) \"\"\"\n",
    "        \n",
    "        vec_product_sum = vec_product.sum(1).sum(1).to(self._device)\n",
    "        if self.debug: print('vec_product_sum.shape:\\t', vec_product_sum.shape)\n",
    "        \"\"\" (batch_size) \"\"\"\n",
    "        \n",
    "        positive_loss = F.logsigmoid(vec_product_sum).to(self._device)\n",
    "        if self.debug: print(\"positive_loss:\\t\", positive_loss)\n",
    "        return positive_loss\n",
    "    \n",
    "    def negative_loss_func(self, t_vec, n_vec):\n",
    "        if self.debug:\n",
    "            print(\"n_vec.shape\", n_vec.shape)\n",
    "            print(\"t_vec.shape\", t_vec.shape)\n",
    "        BMM = torch.einsum('bnhd,bhdp->bnhp', n_vec, t_vec.unsqueeze(3)).to(self._device)\n",
    "        \"\"\" (bs, neg_num, ngram_size, 1)\"\"\"\n",
    "        if self.debug: print('BMM.shape:\\t', BMM.shape)\n",
    "        neg_loss = F.logsigmoid(BMM).squeeze(3).sum(2).sum(1).to(self._device)\n",
    "        return neg_loss\n",
    "        \n",
    "    def loss_func(self, t_vec, c_vec, n_vec):\n",
    "        positive_loss = self.entropy_loss_func(t_vec, c_vec)\n",
    "        negative_loss = self.negative_loss_func(t_vec, n_vec)\n",
    "        \n",
    "#         print('positive_loss: {} negative_loss: {}'.format(positive_loss.neg(), negative_loss.neg()))\n",
    "        if self.debug: print('positive_loss', positive_loss.mean())\n",
    "        total_loss = -(positive_loss + negative_loss).mean()\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def cal_loss(self, outputs, targets=None):\n",
    "        t_vec, c_vec, n_vec = outputs\n",
    "\n",
    "        t_vec = t_vec.float()\n",
    "        c_vec = t_vec.float()\n",
    "        n_vec = n_vec.float()\n",
    "        return self.loss_func(t_vec, c_vec, n_vec)\n",
    "    \n",
    "    def cal_metrics(self, outputs, targets=None):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c03a7b4-fc4d-4df6-ba53-c1b2eed768ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = YahooDataset(max_samples=max_samples, local_dir=\"../datasets/small_yahoo_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47af332c-59cb-4dd6-bea8-abae9aa1ea38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 9000/9000 [00:08<00:00, 1066.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "377\n",
      "995\n"
     ]
    }
   ],
   "source": [
    "subword_hashes = subwordhash(dataset.train)\n",
    "\n",
    "word_num = subword_hashes.word_num\n",
    "max_sw_hash_len = subword_hashes.max_hash\n",
    "max_sample_len = subword_hashes.max_sample\n",
    "print(max_sw_hash_len)\n",
    "print(max_sample_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e4c515d-2a03-4d45-a520-dfc8b1294a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = subwordembedding(num_embeddings = num_emb, \n",
    "                                  embedding_dim = emb_dim, \n",
    "                                  device = device, \n",
    "                                  padding_idx = 0,\n",
    "                                  sumfirst = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2c690a0-ffd7-4366-b91f-b5be99b1a221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessor = Hash_Preprocessor()\n",
    "dataset.train.set_preprocessor(preprocessor)\n",
    "dataset.val.set_preprocessor(preprocessor)\n",
    "dataset.test.set_preprocessor(preprocessor)\n",
    "\n",
    "dataloader_train = DataLoader(dataset.train, batch_size = batch_size, shuffle = True)\n",
    "dataloader_val = DataLoader(dataset.val, batch_size = batch_size, shuffle = False)\n",
    "dataloader_test = DataLoader(dataset.test, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d56ce29c-72c6-4796-a478-cefd410417a1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_dataset = YahooDataset(max_samples=max_samples, local_dir=\"../datasets/small_yahoo_dataset_text\")\n",
    "\n",
    "preprocessor = Word_Preprocessor()\n",
    "word_dataset.train.set_preprocessor(preprocessor)\n",
    "word_dataset.val.set_preprocessor(preprocessor)\n",
    "word_dataset.test.set_preprocessor(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e44c269-a784-40ad-91bb-534f208282b5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 900/900 [01:40<00:00,  8.97it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 32.05it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:47<00:00,  9.27it/s]\n"
     ]
    }
   ],
   "source": [
    "fasttext_train_dataset = FastTextDataset(word_dataset.train, context_size, neg_num, device, uniform)\n",
    "fasttext_val_dataset = FastTextDataset(word_dataset.val, context_size, neg_num, device, uniform)\n",
    "fasttext_test_dataset = FastTextDataset(word_dataset.test, context_size, neg_num, device, uniform)\n",
    "\n",
    "fasttext_loader_train = DataLoader(fasttext_train_dataset, batch_size = batch_size, shuffle = True)\n",
    "fasttext_loader_val = DataLoader(fasttext_val_dataset, batch_size = batch_size, shuffle = False)\n",
    "fasttext_loader_test = DataLoader(fasttext_test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a739acc5-484f-4b63-b62a-33c7deedb735",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = pl.loggers.CSVLogger(\"../Trained_Models/SubwordEmbedding/logs\", name = f\"Fasttext_{emb_dim}_{dist}\")\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath = \"../Trained_Models/SubwordEmbedding/checkpoints\",\n",
    "    filename = 'best_model_latesum',\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'min'\n",
    ")\n",
    "class LitProgressBar(pl.callbacks.ProgressBar):\n",
    "    def init_validation_tqdm(self):\n",
    "        bar = tqdm(disable=True)\n",
    "        return bar\n",
    "bar = LitProgressBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35a25234-21f8-4e56-b2b0-e25af649891a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\device_parser.py:130: LightningDeprecationWarning: Parsing of the Trainer argument gpus='0' (string) will change in the future. In the current version of Lightning, this will select CUDA device with index 0, but from v1.5 it will select gpus [] (same as gpus=0 (int)).\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type             | Params\n",
      "----------------------------------------------------\n",
      "0 | word_embedding | subwordembedding | 100 M \n",
      "----------------------------------------------------\n",
      "100 M     Trainable params\n",
      "0         Non-trainable params\n",
      "100 M     Total params\n",
      "400.000   Total estimated model params size (MB)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e935c5d77143d1ab3cfc8204a2ae6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fasttext_model = FastTextTrainer(word_embedding, \n",
    "                                 device,\n",
    "                                 debug = False)\n",
    "torch.cuda.empty_cache()\n",
    "trainer = pl.Trainer(logger = logger, \n",
    "                     gpus = '0', \n",
    "                     callbacks = [checkpoint, bar], \n",
    "                     num_sanity_val_steps = 0, \n",
    "                     auto_lr_find = True,\n",
    "                     max_epochs = max_epochs)\n",
    "# trainer = pl.Trainer(logger=logger, callbacks=[checkpoint, bar], max_epochs=100)\n",
    "trainer.fit(fasttext_model, \n",
    "            train_dataloader = fasttext_loader_train, \n",
    "            val_dataloaders = fasttext_loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94bb33e0-9729-43aa-862f-736401d7c0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22418829c53462397584e4520d7e3ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 1332.9322509765625}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1332.9322509765625}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(test_dataloaders = fasttext_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8f4df7d-8753-4b1a-920e-dfa02ee0905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(word_embedding.state_dict(), emb_path+'_latesum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fa3e2ef-1343-4f8b-bfe0-8013efd6d09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2070'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f740f01-d2d9-482e-9a00-18bb51d76e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ab'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'a'+'b'"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4508ce6-659c-41f7-9fcd-c582eabb809f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "5048288e-cef2-4859-953a-1fefcdd795a5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61453812-6658-43e7-9cc8-9aa0cf0bc10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 7, 1])\n"
     ]
    }
   ],
   "source": [
    "bs = 3\n",
    "ngr_s = 7\n",
    "ngnum = 2\n",
    "emb_dim = 5\n",
    "\n",
    "neg = torch.randn(bs, ngnum, ngr_s, emb_dim)\n",
    "target = torch.randn(bs, ngr_s, emb_dim)\n",
    "a = torch.einsum('bnhd,bhdp->bnhp', neg, target.unsqueeze(3))\n",
    "# lsm = F.logsigmoid(a).squeeze(3).sum(1).sum(1)\n",
    "# product = torch.mul(target,target)\n",
    "print(a.shape)\n",
    "\n",
    "# neg = torch.randn(bs, ngnum, emb_dim)\n",
    "# target = torch.randn(bs, emb_dim)\n",
    "# b = torch.bmm(neg,target.unsqueeze(2))\n",
    "# b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e03b9-bc70-4a71-813d-bfdbfddc8477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619223dc-e8bf-421d-a5a2-21be3314d561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceee23a7-2b91-45cc-b0a9-3e04d39e30f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a444996-ef22-40c8-8661-133ddeb4797c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class test(Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.embed = Embedding(20, 5)\n",
    "#         self.embed.weight.data.uniform_(-0.05,0.05)\n",
    "        \n",
    "#     def forward(self):\n",
    "#         idx = torch.tensor([0,1], dtype = torch.long)\n",
    "#         multiplier = self.embed(idx)\n",
    "#         target = torch.tensor([[i for i in range(5)]], dtype = torch.long)\n",
    "#         print(\"idx.shape: \", idx.shape)\n",
    "#         print(\"idx: \", idx)\n",
    "#         print(\"\\nmultiplier.shape: \", multiplier.shape)\n",
    "#         print(\"multiplier: \", multiplier)\n",
    "#         print(\"\\ntarget.shape: \", target.shape)\n",
    "#         print(\"target: \", target)\n",
    "        \n",
    "#         product = torch.mul(target, multiplier)\n",
    "#         print(\"\\nproduct.shape: \", product.shape)\n",
    "#         print(\"product: \", product)\n",
    "#         emb_sum = torch.sum(product, dim=1)\n",
    "#         print(\"\\nsum.shape: \", emb_sum.shape)\n",
    "#         print(\"sum: \", emb_sum)\n",
    "        \n",
    "#         noise_dist = torch.ones(20)\n",
    "#         ng = torch.multinomial(noise_dist,5, replacement = True)\n",
    "        \n",
    "#         return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94cfa918-bd44-459f-8c8e-04b1c2483140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(int(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ae26383-d2c5-429d-b58d-ac0b872af98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n100d\\n- uniform: 140 test\\n- noise: \\n50d\\n- uniform 127 test\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "100d\n",
    "- uniform: 140 test\n",
    "- noise: \n",
    "50d\n",
    "- uniform 127 test\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e47259-230f-4f30-b07a-a8007d288163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d032d0-4187-4629-8b0c-5b3f8b530d43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
