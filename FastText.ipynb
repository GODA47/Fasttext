{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "696af1c4-3c51-4fc5-a17d-0b2aa7621f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from nlp_datasets import YahooDataset\n",
    "from nlp_datasets import BaseDataset\n",
    "from nlp_datasets import WordSim353Dataset\n",
    "from nlp_modeltrainers import BaseTrainerModule\n",
    "from nlp_metrics import Metrics\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.nn import Module, Linear, Embedding\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "import fastwer\n",
    "import re\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "dev = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ab17f94-5535-4d5d-b9ed-1cdd8556e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.SubwordHash import subwordhash, Hash_Preprocessor\n",
    "from utils.SubwordHash import Word_Preprocessor\n",
    "from utils.SubwordEmbedding import subwordembedding\n",
    "from utils.config import *\n",
    "from utils.replace_dict import rep\n",
    "from utils.dict_freq import get_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71556256-ec1b-41a0-a298-992e0bde8386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FastTextDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 word_dataset, \n",
    "                 context_size,\n",
    "                 neg_num,\n",
    "                 device,\n",
    "                 uniform = False):\n",
    "        self.word_dataset = word_dataset\n",
    "        self.device = device\n",
    "        w_dict, w_to_id, v_id_f  = self.make_dict()\n",
    "        self.word_dict = w_dict\n",
    "#         self.vocab_freq = v_f\n",
    "        self.vocab_idx_freq = v_id_f\n",
    "        self.word_to_idx = w_to_id\n",
    "        self.context_size = context_size\n",
    "        self.neg_num = neg_num\n",
    "        print('Making instances (t,c,[ns])...')\n",
    "        self.data = np.array(self.get_training_data(context_size, neg_num, uniform), dtype = object)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        target = torch.from_numpy(subword_hashes(self.word_dict[int(self.data[idx,0])])).to(self.device)\n",
    "        context = torch.from_numpy(subword_hashes(self.word_dict[int(self.data[idx,1])])).to(self.device)\n",
    "        negatives = torch.from_numpy(np.array([subword_hashes(self.word_dict[n_idx]) for n_idx in self.data[idx,2]])).to(self.device)\n",
    "            \n",
    "        output = {'input':{ 'target':target, \n",
    "                            'context':context, \n",
    "                            'negatives':negatives},\n",
    "                  'target':[]}\n",
    "        return output\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def make_dict(self):\n",
    "        word_dict = {}\n",
    "        word_to_idx = {}\n",
    "        vocab_freq = {}\n",
    "        count = 0\n",
    "        for sample in self.word_dataset:\n",
    "            for word in sample:\n",
    "                word_dict[count] = word\n",
    "                if word not in vocab_freq:\n",
    "                    vocab_freq[word] = 0\n",
    "                    word_to_idx[word] = len(vocab_freq)-1\n",
    "                vocab_freq[word] += 1\n",
    "                count += 1\n",
    "        vocab_idx_freq = {word_to_idx[w]:vocab_freq[w] for w in vocab_freq}\n",
    "        return word_dict, word_to_idx, vocab_idx_freq\n",
    "    \n",
    "    def get_training_data(self, context_size, neg_num, uniform):\n",
    "        t_pos = 0\n",
    "        training_data = []\n",
    "        for sample in tqdm(self.word_dataset):\n",
    "            for tp in range(len(sample)):\n",
    "                context_pos = []\n",
    "                for sign in [-1,1]:\n",
    "                    for window in range(1, context_size+1):\n",
    "                        c_pos = t_pos + sign*(window)\n",
    "                        if c_pos not in range(len(sample)):\n",
    "                            break\n",
    "                        context_pos.append(c_pos)\n",
    "                \n",
    "                vocab_list, vocab_dist = self.negative_dist(t_pos, context_pos, uniform)\n",
    "                vocab = list(vocab_list)\n",
    "                for c_pos in context_pos:\n",
    "                    negative_idxs = np.random.choice(vocab_list, neg_num, p = vocab_dist, replace = True)\n",
    "                    training_data.append([t_pos, c_pos, negative_idxs])\n",
    "                t_pos += 1\n",
    "        return training_data\n",
    "    \n",
    "    def negative_dist(self, t_pos, c_pos, uniform):\n",
    "        vocab_idx_bag = self.vocab_idx_freq.copy()\n",
    "        exclude_words = []\n",
    "        for pos in c_pos:\n",
    "            exclude_words.append(self.word_to_idx[self.word_dict[pos]])\n",
    "        exclude_words.append(self.word_to_idx[self.word_dict[t_pos]])\n",
    "        exclude_words = set(exclude_words)\n",
    "        for w_idx in exclude_words:\n",
    "            vocab_idx_bag.pop(w_idx)\n",
    "            \n",
    "        if uniform:\n",
    "            noise_dist = np.array([1/len(vocab_idx_bag)]*len(vocab_idx_bag))\n",
    "        else:\n",
    "            word_freq = np.array(list(vocab_idx_bag))\n",
    "            unigram_dist = word_freq/np.sum(word_freq)\n",
    "            noise_dist = unigram_dist**(0.75)/np.sum(unigram_dist**(0.75))\n",
    "            \n",
    "        return list(vocab_idx_bag), noise_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48f22132-c9b0-4dbe-b605-e7352021c626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FastTextTrainer(BaseTrainerModule):\n",
    "    def __init__(self, word_embedding, device, debug = False):\n",
    "        super().__init__()\n",
    "        self.word_embedding = word_embedding\n",
    "        self._device = device\n",
    "        self.debug = debug\n",
    "        \n",
    "    def forward(self, target, context, negatives):\n",
    "        # (batch_size, embedding_dim)\n",
    "        target_vec = self.word_embedding(target)\n",
    "        # (batch_size, embedding_dim)\n",
    "        context_vec = self.word_embedding(context)\n",
    "        # (batch_size, negatives_num, embedding_dim)\n",
    "        negatives_vec = self.word_embedding(negatives)\n",
    "        \n",
    "        return target_vec, context_vec, negatives_vec\n",
    "    \n",
    "    def entropy_loss_func(self, vec1, vec2):\n",
    "        \"\"\"\n",
    "        vec1: (batch_size, embedding_dim)\n",
    "        vec2: (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        if self.debug: \n",
    "            print('vec1.shape:\\t', vec1.shape)\n",
    "            print('vec2.shape:\\t', vec2.shape)\n",
    "            \n",
    "        vec_product = torch.mul(vec1, vec2).to(self._device)\n",
    "        if self.debug: print('vec_product.shape:\\t', vec_product.shape)\n",
    "        if self.debug: print('vec_product:\\t', vec_product)\n",
    "        \"\"\" (batch_size, emb_dim) \"\"\"\n",
    "        \n",
    "        vec_product_sum = vec_product.sum(dim=1).to(self._device)\n",
    "        if self.debug: print('vec_product_sum.shape:\\t', vec_product_sum.shape)\n",
    "        if self.debug: print('vec_product_sum:\\t', vec_product_sum)\n",
    "        \"\"\" (batch_size) \"\"\"\n",
    "        \n",
    "        positive_loss = F.logsigmoid(vec_product_sum).to(self._device)\n",
    "        if self.debug: print(\"positive_loss:\\t\", positive_loss)\n",
    "        return positive_loss\n",
    "    \n",
    "    def negative_loss_func(self, t_vec, n_vec):\n",
    "        BMM = torch.bmm(n_vec.neg(), t_vec.unsqueeze(2)).to(self._device)\n",
    "        \"\"\" (bs, neg_num, 1)\"\"\"\n",
    "#         print('BMM.shape:\\t', BMM.shape)\n",
    "        neg_loss = F.logsigmoid(BMM).squeeze(2).sum(1).to(self._device)\n",
    "        return neg_loss\n",
    "        \n",
    "    def loss_func(self, t_vec, c_vec, n_vec):\n",
    "        positive_loss = self.entropy_loss_func(t_vec, c_vec)\n",
    "        negative_loss = self.negative_loss_func(t_vec, n_vec)\n",
    "        \n",
    "#         print('positive_loss: {} negative_loss: {}'.format(positive_loss.neg(), negative_loss.neg()))\n",
    "        total_loss = -(positive_loss + negative_loss).mean()\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def cal_loss(self, outputs, targets=None):\n",
    "        t_vec, c_vec, n_vec = outputs\n",
    "\n",
    "        t_vec = t_vec.float()\n",
    "        c_vec = t_vec.float()\n",
    "        n_vec = n_vec.float()\n",
    "        return self.loss_func(t_vec, c_vec, n_vec)\n",
    "    \n",
    "    def cal_metrics(self, outputs, targets=None):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64c6220e-ab04-4fcd-aa46-e7170ac16672",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "class Word_Preprocessor:\n",
    "    def __init__(self, freq_dict = {}, train = False, subsampling = False):\n",
    "        self.freq_dict = freq_dict\n",
    "        self.train = train\n",
    "        self.subsampling = subsampling\n",
    "        self.total = sum(list(freq_dict.values()))\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        text = sample[\"input\"]\n",
    "        text = self.prep_text(text)\n",
    "        tokenized = word_tokenize(text.lower())\n",
    "        if self.train:\n",
    "            tokenized = self.cull(tokenized)\n",
    "            if self.subsampling:\n",
    "                tokenized = self.subsample(tokenized)\n",
    "        return [w for w in tokenized]\n",
    "    \n",
    "    def prep_text(self, text):\n",
    "        tknzd = word_tokenize(text)\n",
    "        tk = []\n",
    "        tknn = []\n",
    "        for idx in range(len(tknzd)):\n",
    "            temp = tknzd[idx].replace(\"\\\\n\", \" \")\n",
    "            if '//' not in temp and 'http' not in temp:\n",
    "                tk.append(temp)\n",
    "        for w in tk:\n",
    "            rep_esc = map(re.escape, rep)\n",
    "            pattern = re.compile(\"|\".join(rep_esc))        \n",
    "            te = pattern.sub(lambda match: rep[match.group(0)], w)\n",
    "            te = re.sub('[^0-9a-zA-Z]+', \" \", te)\n",
    "            if stopword and te.lower() in stop_words:\n",
    "                continue\n",
    "            te = lem.lemmatize(te,'v')\n",
    "#             if te[0].isupper():\n",
    "#                 te = \"^\" + te\n",
    "            tknn.append(te)\n",
    "        return \" \".join(tknn).lower()\n",
    "    \n",
    "    def cull(self, tokenized):\n",
    "        culled = []\n",
    "        for w in tokenized:\n",
    "            if self.freq_dict[w] >= min_freq:\n",
    "                culled.append(w)\n",
    "        return culled\n",
    "\n",
    "    def subsample(self, tokenized):\n",
    "        subsamplled = []\n",
    "        for word in tokenized:\n",
    "            freq = self.freq_dict[word]/self.total\n",
    "            p_keep = math.sqrt(threshold/(freq))\n",
    "            keep = np.random.choices([0,1],[1-p_keep, p_keep])\n",
    "            if keep == 1:\n",
    "                subsamplled.append(word)\n",
    "        return subsamplled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aae93cf-c396-44db-a864-5463e5525637",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 900/900 [00:03<00:00, 267.60it/s]\n"
     ]
    }
   ],
   "source": [
    "word_dataset = YahooDataset(max_samples=max_samples, local_dir=\"../datasets/small_yahoo_dataset_text\")\n",
    "freq_dict = get_freq_dict(word_dataset.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14d8feb3-474a-4890-a2b7-292874b34b7c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why has TAVFX fallen so much yesterday?\\nIt appears that the question period has expired. Other answerers deserve to receive the 'best answer' vote for their attempt at answering your question.  If you have received an answer that meets your needs, _please_ choose one of those as a 'best answer' as soon as you can; otherwise, this question will go to an automatic vote. If you haven't received a good answer for your question, you may want to consider the following,\\\\n\\\\n1) Remove this version of your question and re-post your question. Newer questions get more activity on Yahoo! Answers than old ones.\\\\n2) If you do re-post your question, consider why it wasn't answered the first time. Could it be more specific? Could it be worded better? Were there grammatical or spelling errors? Was it in the best category? Can you provide more helpful details?\\\\n\\\\nIf it doesn't seem likely that re-posting your question will help you, then here's a listing of my favorite 'answer sites'. Maybe one of them will help you.\\\\n\\\\nAnswers.com http://www.answers.com/\\\\nBartleby http://www.bartleby.com/\\\\nYahoo Reference http://education.yahoo.com/reference/\\\\nHowStuffWorks http://www.howstuffworks.com/\\\\nWikipedia http://en.wikipedia.org/wiki/Main_Page\\\\n\\\\nSince I really haven't answered your question, it is not necessary to give me any points. This post is simply to encourage you to pick a best answer or to help you find a good answer. \\\\n\\\\nIf you're concerned that I am unfairly voting for my own answer, please review this information first.  It has also been provided to the Y!Answers team.\\\\nhttp://photobucket.com/albums/i154/novell_mcne/ \\\\n\\\\nRegards.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dataset.train[1]['input']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a23167b-0b36-44a0-aa83-d8d91dbf9b38",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "lem = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "t = word_dataset.train[1]['input']\n",
    "tknzd = word_tokenize(t)\n",
    "tk = []\n",
    "for idx in range(len(tknzd)):\n",
    "    temp = tknzd[idx].replace(\"\\\\n\", \" \")\n",
    "    if '//' not in temp and 'http' not in temp:\n",
    "        tk.append(temp)\n",
    "tknn = []\n",
    "for w in tk:\n",
    "    rep_esc = map(re.escape, rep)\n",
    "    pattern = re.compile(\"|\".join(rep_esc))        \n",
    "    te = pattern.sub(lambda match: rep[match.group(0)], w)\n",
    "    te = re.sub('[^0-9a-zA-Z]+', \" \", te)\n",
    "    if stopword and te.lower() in stop_words:\n",
    "        continue\n",
    "    te = lem.lemmatize(te,'v')\n",
    "    if te[0].isupper():\n",
    "        te = \"^\" + te\n",
    "    tknn.append(te)\n",
    "\n",
    "    \n",
    "    \n",
    "\" \".join(tknn).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02cedc4-d986-456b-a46c-363d5850cfb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84263580-c532-4ed8-854a-63efb5d9eb25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25d835e3-09c0-49af-9b63-ea635aa566ba",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'if': 439,\n",
       " 'you': 1220,\n",
       " 'hold': 30,\n",
       " 'it': 1032,\n",
       " 'to': 2160,\n",
       " 'tight': 2,\n",
       " 'will': 272,\n",
       " 'fall': 17,\n",
       " 'out': 173,\n",
       " 'of': 1587,\n",
       " 'your': 418,\n",
       " 'hand': 23,\n",
       " 'loosely': 3,\n",
       " 'break': 14,\n",
       " 'what': 564,\n",
       " 'be': 3131,\n",
       " 'an': 278,\n",
       " 'egg': 12,\n",
       " 'why': 167,\n",
       " 'have': 807,\n",
       " 'tavfx': 1,\n",
       " 'so': 217,\n",
       " 'much': 90,\n",
       " 'yesterday': 1,\n",
       " 'appear': 12,\n",
       " 'that': 948,\n",
       " 'the': 3779,\n",
       " 'question': 94,\n",
       " 'period': 11,\n",
       " 'expire': 4,\n",
       " 'other': 179,\n",
       " 'answerers': 1,\n",
       " 'deserve': 6,\n",
       " 'receive': 14,\n",
       " 'best': 152,\n",
       " 'answer': 110,\n",
       " 'vote': 18,\n",
       " 'for': 807,\n",
       " 'their': 164,\n",
       " 'attempt': 10,\n",
       " 'at': 265,\n",
       " 'meet': 27,\n",
       " 'need': 100,\n",
       " 'please': 23,\n",
       " 'choose': 28,\n",
       " 'one': 960,\n",
       " 'those': 55,\n",
       " 'as': 413,\n",
       " 'a': 1996,\n",
       " 'soon': 14,\n",
       " 'can': 475,\n",
       " 'otherwise': 6,\n",
       " 'this': 361,\n",
       " 'go': 186,\n",
       " 'automatic': 2,\n",
       " 'not': 657,\n",
       " 'good': 149,\n",
       " 'may': 71,\n",
       " 'want': 148,\n",
       " 'consider': 40,\n",
       " 'follow': 31,\n",
       " 'remove': 14,\n",
       " 'version': 14,\n",
       " 'and': 1672,\n",
       " 're': 74,\n",
       " 'post': 16,\n",
       " 'newer': 1,\n",
       " 'get': 295,\n",
       " 'more': 238,\n",
       " 'activity': 4,\n",
       " 'on': 563,\n",
       " 'yahoo': 70,\n",
       " 'answers': 12,\n",
       " 'than': 129,\n",
       " 'old': 31,\n",
       " 'ones': 24,\n",
       " 'two': 588,\n",
       " 'do': 726,\n",
       " 'first': 100,\n",
       " 'time': 184,\n",
       " 'could': 107,\n",
       " 'specific': 11,\n",
       " 'word': 56,\n",
       " 'better': 75,\n",
       " 'were': 4,\n",
       " 'there': 281,\n",
       " 'grammatical': 2,\n",
       " 'or': 495,\n",
       " 'spell': 13,\n",
       " 'errors': 1,\n",
       " 'was': 8,\n",
       " 'in': 1233,\n",
       " 'category': 8,\n",
       " 'provide': 29,\n",
       " 'helpful': 9,\n",
       " 'detail': 12,\n",
       " 'seem': 41,\n",
       " 'likely': 27,\n",
       " 'posting': 1,\n",
       " 'help': 89,\n",
       " 'then': 131,\n",
       " 'here': 63,\n",
       " 's': 518,\n",
       " 'list': 41,\n",
       " 'my': 238,\n",
       " 'favorite': 16,\n",
       " 'sit': 32,\n",
       " 'maybe': 21,\n",
       " 'them': 147,\n",
       " 'com': 66,\n",
       " 'reference': 8,\n",
       " 'i': 1110,\n",
       " 'really': 87,\n",
       " 'necessary': 6,\n",
       " 'give': 75,\n",
       " 'me': 106,\n",
       " 'any': 134,\n",
       " 'point': 47,\n",
       " 'simply': 20,\n",
       " 'encourage': 9,\n",
       " 'pick': 28,\n",
       " 'find': 158,\n",
       " 'concern': 9,\n",
       " 'unfairly': 1,\n",
       " 'own': 61,\n",
       " 'review': 6,\n",
       " 'information': 47,\n",
       " 'also': 143,\n",
       " 'y': 17,\n",
       " 'regards': 1,\n",
       " 'people': 155,\n",
       " 'love': 54,\n",
       " 'oprah': 4,\n",
       " 'because': 88,\n",
       " 'she': 65,\n",
       " 'personify': 1,\n",
       " 'classic': 6,\n",
       " 'cinderella': 1,\n",
       " 'story': 7,\n",
       " 'american': 25,\n",
       " 'dream': 6,\n",
       " 'overcome': 3,\n",
       " 'poverty': 7,\n",
       " 'abuse': 1,\n",
       " 'become': 40,\n",
       " 'huge': 11,\n",
       " 'tv': 20,\n",
       " 'success': 5,\n",
       " 'very': 106,\n",
       " 'charismatic': 1,\n",
       " 'like': 209,\n",
       " 'stuff': 18,\n",
       " 'away': 24,\n",
       " 'baby': 18,\n",
       " 'breath': 4,\n",
       " 'water': 50,\n",
       " 'inside': 8,\n",
       " 'mother': 7,\n",
       " 'womb': 3,\n",
       " 'umbilical': 2,\n",
       " 'cord': 5,\n",
       " 'use': 228,\n",
       " 'when': 189,\n",
       " 'lungs': 2,\n",
       " 'fill': 9,\n",
       " 'with': 434,\n",
       " 'embryonic': 1,\n",
       " 'fluid': 2,\n",
       " 'but': 328,\n",
       " 'he': 186,\n",
       " 'isnt': 3,\n",
       " 'breathing': 1,\n",
       " 'we': 191,\n",
       " 'would': 211,\n",
       " 'thing': 35,\n",
       " 'everything': 17,\n",
       " 'live': 52,\n",
       " 'while': 55,\n",
       " 'receieved': 1,\n",
       " 'through': 66,\n",
       " 'include': 59,\n",
       " 'oxygen': 13,\n",
       " 'environment': 8,\n",
       " 'republicans': 2,\n",
       " 'definition': 11,\n",
       " 'curse': 2,\n",
       " 'true': 20,\n",
       " 'tune': 5,\n",
       " 'violin': 1,\n",
       " 'c': 46,\n",
       " 'string': 6,\n",
       " 'mean': 133,\n",
       " 'add': 33,\n",
       " 'another': 50,\n",
       " 'example': 47,\n",
       " 'just': 157,\n",
       " 'latter': 1,\n",
       " 'yes': 37,\n",
       " 'doable': 1,\n",
       " 'economics': 2,\n",
       " 'run': 50,\n",
       " 'restaurant': 6,\n",
       " 'running': 1,\n",
       " 'look': 154,\n",
       " 'hard': 31,\n",
       " 'work': 114,\n",
       " 'long': 57,\n",
       " 'hours': 28,\n",
       " 'xd': 28,\n",
       " 'br': 121,\n",
       " 'percentage': 5,\n",
       " 'restaurants': 2,\n",
       " 'profitable': 1,\n",
       " 'average': 32,\n",
       " 'revenue': 5,\n",
       " 'net': 12,\n",
       " 'income': 6,\n",
       " 'small': 26,\n",
       " 'nine': 287,\n",
       " 'zero': 816,\n",
       " 'fail': 7,\n",
       " 'year': 68,\n",
       " 'existence': 7,\n",
       " 'profit': 7,\n",
       " 'level': 40,\n",
       " 'define': 16,\n",
       " 'without': 40,\n",
       " 'know': 157,\n",
       " 'food': 25,\n",
       " 'concept': 4,\n",
       " 'type': 32,\n",
       " 'liquor': 1,\n",
       " 'license': 5,\n",
       " 'establishment': 2,\n",
       " 'beer': 2,\n",
       " 'wine': 3,\n",
       " 'full': 28,\n",
       " 'bar': 4,\n",
       " 'depend': 38,\n",
       " 'heavily': 1,\n",
       " 'take': 123,\n",
       " 'order': 19,\n",
       " 'mostly': 9,\n",
       " 'din': 2,\n",
       " 'within': 18,\n",
       " 'five': 281,\n",
       " 'day': 52,\n",
       " 'yo': 4,\n",
       " 'serve': 7,\n",
       " 'lunch': 4,\n",
       " 'dinner': 4,\n",
       " 'how': 338,\n",
       " 'many': 110,\n",
       " 'members': 13,\n",
       " 'german': 6,\n",
       " 'bundestag': 3,\n",
       " 'consist': 1,\n",
       " 'seats': 1,\n",
       " 'regularly': 4,\n",
       " 'eight': 179,\n",
       " 'however': 37,\n",
       " 'additional': 11,\n",
       " 'seat': 4,\n",
       " 'call': 89,\n",
       " 'ueberhangmandate': 1,\n",
       " 'currently': 13,\n",
       " 'six': 194,\n",
       " 'four': 255,\n",
       " 'co': 10,\n",
       " 'worker': 3,\n",
       " 'white': 11,\n",
       " 'head': 19,\n",
       " 'thats': 4,\n",
       " 'ready': 7,\n",
       " 'pop': 8,\n",
       " 'should': 153,\n",
       " 'able': 46,\n",
       " 'over': 74,\n",
       " 'him': 93,\n",
       " 'clean': 15,\n",
       " 'up': 162,\n",
       " 'mess': 6,\n",
       " 'is': 76,\n",
       " 'such': 66,\n",
       " 'technical': 9,\n",
       " 'temp': 6,\n",
       " 'agency': 3,\n",
       " 'agencies': 7,\n",
       " 'hr': 1,\n",
       " 'low': 21,\n",
       " 'skill': 2,\n",
       " 'job': 36,\n",
       " 'somebody': 6,\n",
       " 'program': 73,\n",
       " 'system': 35,\n",
       " 'admin': 2,\n",
       " 'process': 26,\n",
       " 'improvement': 5,\n",
       " 'etc': 44,\n",
       " 'definitely': 12,\n",
       " 'hire': 4,\n",
       " 'employees': 5,\n",
       " 'skills': 3,\n",
       " 'describe': 16,\n",
       " 'especially': 15,\n",
       " 'areas': 12,\n",
       " 'where': 130,\n",
       " 'plentiful': 1,\n",
       " 'suggest': 15,\n",
       " 'conduct': 4,\n",
       " 'search': 28,\n",
       " 'area': 45,\n",
       " 'contact': 22,\n",
       " 'local': 16,\n",
       " 'employment': 4,\n",
       " 'recruiter': 1,\n",
       " 'ask': 59,\n",
       " 'about': 220,\n",
       " 'locale': 1,\n",
       " 'date': 37,\n",
       " 'riddle': 8,\n",
       " 'happen': 25,\n",
       " 'again': 20,\n",
       " 'years': 71,\n",
       " 'even': 80,\n",
       " 'read': 36,\n",
       " 'upside': 2,\n",
       " 'down': 65,\n",
       " 'reamin': 1,\n",
       " 'miles': 28,\n",
       " 'north': 23,\n",
       " 'east': 6,\n",
       " 'south': 14,\n",
       " 'seven': 174,\n",
       " 'west': 9,\n",
       " 'far': 25,\n",
       " 'direction': 7,\n",
       " 'from': 335,\n",
       " 'start': 69,\n",
       " 'trouble': 4,\n",
       " 'no': 154,\n",
       " 'explanation': 12,\n",
       " 'who': 176,\n",
       " 'believe': 40,\n",
       " 'since': 46,\n",
       " 'each': 67,\n",
       " 'once': 30,\n",
       " 'move': 27,\n",
       " 'matter': 16,\n",
       " 'draw': 4,\n",
       " 'grid': 3,\n",
       " 'due': 21,\n",
       " 'come': 91,\n",
       " 'back': 76,\n",
       " 'three': 307,\n",
       " 'right': 92,\n",
       " 'triangle': 5,\n",
       " 'leave': 40,\n",
       " 'hypotenuse': 1,\n",
       " 'whose': 7,\n",
       " 'side': 30,\n",
       " 'pythagoras': 1,\n",
       " 'b': 32,\n",
       " 'square': 18,\n",
       " 'root': 15,\n",
       " 'final': 8,\n",
       " 'distance': 20,\n",
       " 'nw': 1,\n",
       " 'airspeed': 1,\n",
       " 'velocity': 3,\n",
       " 'unladen': 2,\n",
       " 'swallow': 6,\n",
       " 'try': 88,\n",
       " 'see': 131,\n",
       " 'obviously': 8,\n",
       " 'monty': 3,\n",
       " 'pyhton': 1,\n",
       " 'fan': 12,\n",
       " 'python': 6,\n",
       " 'holy': 3,\n",
       " 'grail': 2,\n",
       " 'bridgekeeper': 3,\n",
       " 'quest': 3,\n",
       " 'arthur': 7,\n",
       " 'seek': 8,\n",
       " 'air': 28,\n",
       " 'speed': 25,\n",
       " 'african': 2,\n",
       " 'european': 14,\n",
       " 'huh': 3,\n",
       " 'auuuuuuuugh': 1,\n",
       " 'bedevere': 1,\n",
       " 'well': 92,\n",
       " 'these': 67,\n",
       " 'things': 47,\n",
       " 'king': 17,\n",
       " 'are': 20,\n",
       " 'humans': 12,\n",
       " 'still': 58,\n",
       " 'evolve': 4,\n",
       " 'seems': 3,\n",
       " 'vast': 3,\n",
       " 'majority': 9,\n",
       " 'enough': 26,\n",
       " 'procreate': 1,\n",
       " 'natural': 13,\n",
       " 'selection': 4,\n",
       " 'dead': 5,\n",
       " 'sleepy': 1,\n",
       " 'attribute': 2,\n",
       " 'select': 11,\n",
       " 'against': 19,\n",
       " 'days': 24,\n",
       " 'evolution': 17,\n",
       " 'ever': 34,\n",
       " 'present': 16,\n",
       " 'always': 52,\n",
       " 'argue': 3,\n",
       " 'rate': 21,\n",
       " 'function': 15,\n",
       " 'change': 38,\n",
       " 'drive': 19,\n",
       " 'human': 25,\n",
       " 'part': 46,\n",
       " 'equation': 5,\n",
       " 'extreme': 2,\n",
       " 'organism': 1,\n",
       " 'escape': 5,\n",
       " 'previous': 8,\n",
       " 'mention': 14,\n",
       " 'now': 85,\n",
       " 'ability': 8,\n",
       " 'easily': 11,\n",
       " 'around': 67,\n",
       " 'world': 75,\n",
       " 'bad': 35,\n",
       " 'indeed': 4,\n",
       " 'slow': 15,\n",
       " 'some': 203,\n",
       " 'cataclysmic': 1,\n",
       " 'affect': 13,\n",
       " 'whole': 10,\n",
       " 'planet': 9,\n",
       " 'kill': 9,\n",
       " 'species': 11,\n",
       " 'completely': 12,\n",
       " 'certainly': 4,\n",
       " 'evolutionary': 6,\n",
       " 'compare': 10,\n",
       " 'race': 10,\n",
       " 'few': 55,\n",
       " 'generations': 1,\n",
       " 'either': 26,\n",
       " 'say': 118,\n",
       " 'event': 6,\n",
       " 'given': 4,\n",
       " 'result': 25,\n",
       " 'genetic': 6,\n",
       " 'mutation': 2,\n",
       " 'favoritism': 1,\n",
       " 'during': 29,\n",
       " 'sexual': 4,\n",
       " 'reproduction': 1,\n",
       " 'desirable': 2,\n",
       " 'sex': 15,\n",
       " 'set': 40,\n",
       " 'path': 3,\n",
       " 'minimize': 1,\n",
       " 'by': 280,\n",
       " 'our': 54,\n",
       " 'control': 21,\n",
       " 'height': 12,\n",
       " 'grow': 13,\n",
       " 'taller': 8,\n",
       " 'western': 6,\n",
       " 'cultural': 1,\n",
       " 'bias': 2,\n",
       " 'woman': 22,\n",
       " 'tall': 5,\n",
       " 'mate': 2,\n",
       " 'note': 17,\n",
       " 'think': 168,\n",
       " 'culturally': 1,\n",
       " 'dependent': 3,\n",
       " 'lead': 24,\n",
       " 'term': 43,\n",
       " 'breed': 4,\n",
       " 'prospect': 1,\n",
       " 'individual': 12,\n",
       " 'conceive': 2,\n",
       " 'parent': 9,\n",
       " 'cabotage': 1,\n",
       " 'cabbage': 1,\n",
       " 'patch': 2,\n",
       " 'kid': 24,\n",
       " 'life': 60,\n",
       " 'rule': 10,\n",
       " 'throw': 11,\n",
       " 'ball': 14,\n",
       " 'football': 13,\n",
       " 'usually': 27,\n",
       " 'quarterback': 1,\n",
       " 'option': 23,\n",
       " 'play': 57,\n",
       " 'someone': 60,\n",
       " 'else': 21,\n",
       " 'anyone': 35,\n",
       " 'they': 337,\n",
       " 'behind': 15,\n",
       " 'line': 25,\n",
       " 'scrimmage': 2,\n",
       " 'players': 14,\n",
       " 'allow': 41,\n",
       " 'player': 19,\n",
       " 'constitutional': 2,\n",
       " 'america': 8,\n",
       " 'god': 34,\n",
       " 'constitution': 5,\n",
       " 'forbid': 2,\n",
       " 'make': 195,\n",
       " 'laws': 7,\n",
       " 'religion': 17,\n",
       " 'which': 191,\n",
       " 'pretty': 28,\n",
       " 'same': 65,\n",
       " 'all': 187,\n",
       " 'persons': 3,\n",
       " 'freedom': 2,\n",
       " 'worship': 3,\n",
       " 'sure': 54,\n",
       " 'keep': 56,\n",
       " 'closest': 1,\n",
       " 'declaration': 2,\n",
       " 'independence': 6,\n",
       " 'endow': 1,\n",
       " 'creator': 3,\n",
       " 'market': 19,\n",
       " 'share': 46,\n",
       " 'gm': 1,\n",
       " 'ford': 2,\n",
       " 'together': 24,\n",
       " 'usa': 9,\n",
       " 'article': 17,\n",
       " 'u': 47,\n",
       " 'automakers': 1,\n",
       " 'combine': 7,\n",
       " 'shrink': 2,\n",
       " 'new': 88,\n",
       " 'percent': 3,\n",
       " 'last': 39,\n",
       " 'month': 17,\n",
       " 'ago': 21,\n",
       " 'consumers': 3,\n",
       " 'snap': 4,\n",
       " 'light': 61,\n",
       " 'truck': 3,\n",
       " 'suvs': 1,\n",
       " 'fluctuate': 1,\n",
       " 'little': 39,\n",
       " 'reason': 41,\n",
       " 'increase': 25,\n",
       " 'divorce': 8,\n",
       " 'most': 130,\n",
       " 'common': 21,\n",
       " 'parameter': 1,\n",
       " 'cause': 37,\n",
       " 'couple': 21,\n",
       " 'separate': 13,\n",
       " 'imo': 2,\n",
       " 'complex': 8,\n",
       " 'equivalents': 1,\n",
       " 'thus': 16,\n",
       " 'marry': 13,\n",
       " 'different': 71,\n",
       " 'directions': 7,\n",
       " 'value': 26,\n",
       " 'easy': 28,\n",
       " 'acceptable': 3,\n",
       " 'quick': 6,\n",
       " 'dispose': 1,\n",
       " 'longer': 16,\n",
       " 'surprise': 9,\n",
       " 'too': 45,\n",
       " 'distant': 1,\n",
       " 'future': 12,\n",
       " 'length': 4,\n",
       " 'marriage': 9,\n",
       " 'drop': 18,\n",
       " 'number': 60,\n",
       " 'marriages': 3,\n",
       " 'per': 20,\n",
       " 'person': 56,\n",
       " 'parallel': 4,\n",
       " 'circle': 3,\n",
       " 'friends': 27,\n",
       " 'often': 34,\n",
       " 'join': 5,\n",
       " 'others': 25,\n",
       " 'grade': 8,\n",
       " 'school': 69,\n",
       " 'high': 61,\n",
       " 'college': 29,\n",
       " 'nomadic': 1,\n",
       " 'car': 30,\n",
       " 'house': 36,\n",
       " 'insurance': 3,\n",
       " 'company': 51,\n",
       " 'recommend': 35,\n",
       " 'california': 21,\n",
       " 'experience': 31,\n",
       " 'costco': 2,\n",
       " 'rat': 18,\n",
       " 'service': 37,\n",
       " 'certain': 17,\n",
       " 'jumpstart': 2,\n",
       " 'friend': 21,\n",
       " 'only': 103,\n",
       " 'tow': 2,\n",
       " 'power': 36,\n",
       " 'auto': 2,\n",
       " 'assitance': 1,\n",
       " 'send': 25,\n",
       " 'quickly': 6,\n",
       " 'whether': 21,\n",
       " 'problem': 24,\n",
       " 'fix': 6,\n",
       " 'highly': 7,\n",
       " 'longest': 4,\n",
       " 'english': 29,\n",
       " 'vowel': 2,\n",
       " 'rhythm': 1,\n",
       " 'below': 38,\n",
       " 'web': 43,\n",
       " 'site': 59,\n",
       " 'fun': 13,\n",
       " 'facts': 5,\n",
       " 'index': 7,\n",
       " 'table': 7,\n",
       " 'contants': 1,\n",
       " 'convienent': 1,\n",
       " 'engines': 3,\n",
       " 'google': 40,\n",
       " 'something': 63,\n",
       " 'course': 32,\n",
       " 'approximate': 1,\n",
       " 'problems': 15,\n",
       " 'every': 46,\n",
       " 'second': 33,\n",
       " 'content': 11,\n",
       " 'de': 7,\n",
       " 'centralized': 1,\n",
       " 'page': 36,\n",
       " 'pass': 23,\n",
       " 'central': 8,\n",
       " 'authority': 5,\n",
       " 'indexer': 1,\n",
       " 'contents': 1,\n",
       " 'compiler': 1,\n",
       " 'crawl': 1,\n",
       " 'pages': 2,\n",
       " 'rest': 22,\n",
       " 'family': 28,\n",
       " 'bird': 16,\n",
       " 'flu': 8,\n",
       " 'frighten': 2,\n",
       " 'disease': 7,\n",
       " 'entire': 13,\n",
       " 'deal': 20,\n",
       " 'epidemic': 1,\n",
       " 'diseases': 1,\n",
       " 'pandemics': 2,\n",
       " 'trace': 3,\n",
       " 'mutate': 2,\n",
       " 'virus': 2,\n",
       " 'score': 19,\n",
       " 'panic': 1,\n",
       " 'stem': 5,\n",
       " 'fact': 13,\n",
       " 'vaccine': 1,\n",
       " 'h': 9,\n",
       " 'fiven': 1,\n",
       " 'asia': 4,\n",
       " 'vaccines': 1,\n",
       " 'regular': 14,\n",
       " 'short': 20,\n",
       " 'supply': 9,\n",
       " 'under': 35,\n",
       " 'normal': 15,\n",
       " 'circumstances': 1,\n",
       " 'hit': 13,\n",
       " 'feverish': 1,\n",
       " 'achey': 1,\n",
       " 'fatigue': 1,\n",
       " 'strike': 7,\n",
       " 'vulnerable': 2,\n",
       " 'elderly': 1,\n",
       " 'young': 7,\n",
       " 'respiratory': 1,\n",
       " 'lower': 29,\n",
       " 'dropout': 3,\n",
       " 'schools': 6,\n",
       " 'curb': 1,\n",
       " 'national': 15,\n",
       " 'prevention': 2,\n",
       " 'center': 15,\n",
       " 'advocate': 3,\n",
       " 'implementation': 3,\n",
       " 'alternative': 9,\n",
       " 'strategies': 2,\n",
       " 'base': 48,\n",
       " 'principle': 2,\n",
       " 'continuous': 1,\n",
       " 'place': 51,\n",
       " 'policies': 2,\n",
       " 'support': 33,\n",
       " 'student': 15,\n",
       " 'retention': 2,\n",
       " 'organization': 8,\n",
       " 'qualifier': 1,\n",
       " 'meaningful': 2,\n",
       " 'education': 22,\n",
       " 'solely': 2,\n",
       " 'performance': 10,\n",
       " 'learner': 2,\n",
       " 'begin': 23,\n",
       " 'emotionally': 5,\n",
       " 'socially': 2,\n",
       " 'cognitively': 1,\n",
       " 'behaviorally': 1,\n",
       " 'factor': 25,\n",
       " 'role': 9,\n",
       " 'learn': 43,\n",
       " 'standard': 19,\n",
       " 'apply': 12,\n",
       " 'ndpc': 1,\n",
       " 'dropouts': 1,\n",
       " 'prevent': 13,\n",
       " 'child': 7,\n",
       " 'assess': 1,\n",
       " 'invidividual': 1,\n",
       " 'progress': 3,\n",
       " 'weigh': 13,\n",
       " 'educational': 5,\n",
       " 'learning': 5,\n",
       " 'theory': 28,\n",
       " 'connection': 10,\n",
       " 'hands': 1,\n",
       " 'academic': 2,\n",
       " 'incorporate': 2,\n",
       " 'community': 12,\n",
       " 'learners': 1,\n",
       " 'show': 42,\n",
       " 'grasp': 1,\n",
       " 'concepts': 2,\n",
       " 'greater': 7,\n",
       " 'experiential': 1,\n",
       " 'instead': 23,\n",
       " 'traditional': 8,\n",
       " 'textbook': 1,\n",
       " 'lecture': 1,\n",
       " 'text': 9,\n",
       " 'hope': 31,\n",
       " 'skin': 4,\n",
       " 'secrete': 3,\n",
       " 'substance': 1,\n",
       " 'bleach': 1,\n",
       " 'materials': 9,\n",
       " 'towel': 1,\n",
       " 'glow': 4,\n",
       " 'blacklight': 1,\n",
       " 'perfume': 4,\n",
       " 'corrode': 1,\n",
       " 'metal': 4,\n",
       " 'acids': 2,\n",
       " 'organic': 1,\n",
       " 'interfere': 3,\n",
       " 'fabric': 1,\n",
       " 'colour': 2,\n",
       " 'nonpermanent': 1,\n",
       " 'acne': 1,\n",
       " 'haha': 1,\n",
       " 'stress': 13,\n",
       " 'lot': 62,\n",
       " 'sleep': 12,\n",
       " 'eat': 25,\n",
       " 'foods': 4,\n",
       " 'fatty': 1,\n",
       " 'wash': 1,\n",
       " 'face': 15,\n",
       " 'least': 23,\n",
       " 'daily': 4,\n",
       " 'neutrogena': 1,\n",
       " 'great': 42,\n",
       " 'cleaning': 3,\n",
       " 'products': 11,\n",
       " 'smile': 3,\n",
       " 'mirror': 8,\n",
       " 'elevator': 9,\n",
       " 'history': 28,\n",
       " 'purpose': 8,\n",
       " 'functional': 2,\n",
       " 'psychological': 2,\n",
       " 'rather': 14,\n",
       " 'mechanical': 5,\n",
       " 'early': 24,\n",
       " 'industrial': 1,\n",
       " 'age': 22,\n",
       " 'build': 33,\n",
       " 'spring': 10,\n",
       " 'coast': 2,\n",
       " 'anything': 23,\n",
       " 'before': 50,\n",
       " 'elevators': 11,\n",
       " 'darn': 3,\n",
       " 'constantly': 5,\n",
       " 'complain': 6,\n",
       " 'challenge': 8,\n",
       " 'typical': 5,\n",
       " 'statement': 10,\n",
       " 'off': 66,\n",
       " 'design': 23,\n",
       " 'faster': 10,\n",
       " 'safer': 2,\n",
       " 'expensive': 15,\n",
       " 'several': 28,\n",
       " 'propose': 5,\n",
       " 'name': 75,\n",
       " 'approach': 7,\n",
       " 'fundamentals': 2,\n",
       " 'est': 4,\n",
       " 'tool': 7,\n",
       " 'engineer': 10,\n",
       " 'fine': 7,\n",
       " 'crazy': 2,\n",
       " 'insert': 2,\n",
       " 'into': 85,\n",
       " 'team': 39,\n",
       " 'angle': 4,\n",
       " 'ideas': 13,\n",
       " 'concentrate': 4,\n",
       " 'larger': 8,\n",
       " 'motor': 2,\n",
       " 'slicker': 1,\n",
       " 'pulley': 1,\n",
       " 'passenger': 1,\n",
       " 'snowball': 2,\n",
       " 'distract': 3,\n",
       " 'comfortable': 6,\n",
       " 'customers': 8,\n",
       " 'scar': 3,\n",
       " 'heights': 1,\n",
       " 'customer': 9,\n",
       " 'research': 16,\n",
       " 'slower': 1,\n",
       " 'actually': 38,\n",
       " 'discover': 6,\n",
       " 'exaggerate': 1,\n",
       " 'sense': 13,\n",
       " 'nothing': 12,\n",
       " 'stare': 2,\n",
       " 'wall': 14,\n",
       " 'safety': 3,\n",
       " 'suspend': 1,\n",
       " 'preoccupy': 2,\n",
       " 'fear': 8,\n",
       " 't': 20,\n",
       " 'room': 17,\n",
       " 'equipment': 9,\n",
       " 'sort': 3,\n",
       " 'brainstorm': 1,\n",
       " 'idea': 17,\n",
       " 'besides': 6,\n",
       " 'danger': 3,\n",
       " 'hair': 2,\n",
       " 'comb': 2,\n",
       " 'properly': 5,\n",
       " 'did': 9,\n",
       " 'her': 45,\n",
       " 'makeup': 2,\n",
       " 'okay': 2,\n",
       " 'instal': 7,\n",
       " 'survey': 2,\n",
       " 'comment': 7,\n",
       " 'though': 42,\n",
       " 'exactly': 13,\n",
       " 'itself': 13,\n",
       " 'us': 63,\n",
       " 'astronaut': 1,\n",
       " 'land': 8,\n",
       " 'moon': 26,\n",
       " 'edwin': 2,\n",
       " 'eugene': 2,\n",
       " 'buzz': 1,\n",
       " 'aldrin': 2,\n",
       " 'jr': 5,\n",
       " 'july': 5,\n",
       " 'foot': 9,\n",
       " 'neil': 1,\n",
       " 'armstrong': 2,\n",
       " 'surface': 21,\n",
       " 'ceremonies': 1,\n",
       " 'rock': 11,\n",
       " 'collect': 5,\n",
       " 'plz': 1,\n",
       " 'relate': 10,\n",
       " 'oracle': 3,\n",
       " 'bout': 2,\n",
       " 'rollup': 1,\n",
       " 'operations': 4,\n",
       " 'shivani': 1,\n",
       " 'pathankot': 1,\n",
       " 'punjabi': 1,\n",
       " 'info': 10,\n",
       " 'gain': 16,\n",
       " 'road': 14,\n",
       " 'motorcycle': 1,\n",
       " 'trail': 3,\n",
       " 'throughout': 6,\n",
       " 'ca': 48,\n",
       " 'hear': 33,\n",
       " 'mojave': 1,\n",
       " 'amaze': 7,\n",
       " 'online': 30,\n",
       " 'masons': 2,\n",
       " 'satanic': 7,\n",
       " 'semantics': 1,\n",
       " 'freemasonry': 2,\n",
       " 'way': 91,\n",
       " 'might': 54,\n",
       " 'freemasons': 1,\n",
       " 'inherently': 1,\n",
       " 'religious': 6,\n",
       " 'connections': 7,\n",
       " 'status': 7,\n",
       " 'society': 10,\n",
       " 'directly': 7,\n",
       " 'coorelating': 1,\n",
       " 'position': 11,\n",
       " 'church': 21,\n",
       " 'choice': 14,\n",
       " 'claim': 8,\n",
       " 'further': 8,\n",
       " 'study': 14,\n",
       " 'organize': 4,\n",
       " 'religions': 3,\n",
       " 'origins': 4,\n",
       " 'link': 43,\n",
       " 'twofold': 2,\n",
       " 'called': 5,\n",
       " 'faith': 4,\n",
       " 'culture': 9,\n",
       " 'thereby': 1,\n",
       " 'suspicion': 1,\n",
       " 'influence': 4,\n",
       " 'freemason': 1,\n",
       " 'rites': 2,\n",
       " 'rituals': 1,\n",
       " 'similar': 17,\n",
       " 'witchcraft': 1,\n",
       " 'lesser': 1,\n",
       " 'hermetics': 1,\n",
       " 'general': 25,\n",
       " 'unknowing': 1,\n",
       " 'public': 32,\n",
       " 'truth': 7,\n",
       " 'never': 34,\n",
       " 'confirm': 3,\n",
       " 'recieve': 3,\n",
       " 'masonic': 2,\n",
       " 'yourself': 25,\n",
       " 'furthermore': 3,\n",
       " 'member': 3,\n",
       " 'knowledge': 4,\n",
       " 'brotherhood': 1,\n",
       " 'secrets': 3,\n",
       " 'societies': 1,\n",
       " 'replete': 1,\n",
       " 'whisper': 1,\n",
       " 'foul': 2,\n",
       " 'undertones': 1,\n",
       " 'godless': 1,\n",
       " 'godlike': 1,\n",
       " 'action': 16,\n",
       " 'beliefs': 5,\n",
       " 'lie': 10,\n",
       " 'understand': 32,\n",
       " 'subject': 7,\n",
       " 'curiosity': 1,\n",
       " 'beneath': 7,\n",
       " 'accurate': 5,\n",
       " 'luck': 18,\n",
       " 'float': 1,\n",
       " 'wood': 14,\n",
       " 'ala': 1,\n",
       " 'bread': 2,\n",
       " 'apples': 3,\n",
       " 'cider': 1,\n",
       " 'gravy': 1,\n",
       " 'cherries': 1,\n",
       " 'mud': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c56a8715-febd-4a5d-a68d-d39ad239da90",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_preprocessor = Word_Preprocessor(freq_dict = freq_dict, \n",
    "                                       train = True,\n",
    "                                       subsampling = subsampling)\n",
    "word_dataset.train.set_preprocessor(train_preprocessor)\n",
    "\n",
    "preprocessor = Word_Preprocessor(freq_dict = freq_dict, \n",
    "                                 train = False,\n",
    "                                 subsampling = subsampling)\n",
    "word_dataset.val.set_preprocessor(preprocessor)\n",
    "word_dataset.test.set_preprocessor(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47af332c-59cb-4dd6-bea8-abae9aa1ea38",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 900/900 [00:01<00:00, 486.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356\n",
      "811\n"
     ]
    }
   ],
   "source": [
    "subword_hashes = subwordhash(word_dataset.train)\n",
    "\n",
    "word_num = subword_hashes.word_num\n",
    "max_sw_hash_len = subword_hashes.max_hash\n",
    "max_sample_len = subword_hashes.max_sample\n",
    "print(max_sw_hash_len)\n",
    "print(max_sample_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49d71893-8e01-4786-ba79-6937fb092837",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what',\n",
       " 's',\n",
       " 'the',\n",
       " 'longest',\n",
       " 'english',\n",
       " 'word',\n",
       " 'without',\n",
       " 'a',\n",
       " 'vowel',\n",
       " 'in',\n",
       " 'it',\n",
       " 'and',\n",
       " 'what',\n",
       " 'do',\n",
       " 'that',\n",
       " 'word',\n",
       " 'mean',\n",
       " 'if',\n",
       " 'it',\n",
       " 's',\n",
       " 'not',\n",
       " 'a',\n",
       " 'common',\n",
       " 'word',\n",
       " 'the',\n",
       " 'longest',\n",
       " 'word',\n",
       " 'without',\n",
       " 'a',\n",
       " 'vowel',\n",
       " 'be',\n",
       " 'rhythm',\n",
       " 'it',\n",
       " 'be',\n",
       " 'reference',\n",
       " 'on',\n",
       " 'below',\n",
       " 'web',\n",
       " 'site',\n",
       " 'where',\n",
       " 'you',\n",
       " 'can',\n",
       " 'find',\n",
       " 'more',\n",
       " 'fun',\n",
       " 'facts']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dataset.train[20]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d6612940-33a7-4693-8364-fe4bab36b36b",
   "metadata": {
    "tags": []
   },
   "source": [
    "dataset = YahooDataset(max_samples=max_samples, local_dir=\"../datasets/small_yahoo_dataset\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74b0cd96-ce74-4c6c-8706-2097272fa8ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "dataset.train[2]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d76fd093-1eaa-4e21-9b6b-2774b9ed0023",
   "metadata": {
    "tags": []
   },
   "source": [
    "preprocessor = Hash_Preprocessor(culled_dict, min_freq)\n",
    "dataset.train.set_preprocessor(preprocessor)\n",
    "dataset.val.set_preprocessor(preprocessor)\n",
    "dataset.test.set_preprocessor(preprocessor)\n",
    "\n",
    "dataloader_train = DataLoader(dataset.train, batch_size = batch_size, shuffle = True)\n",
    "dataloader_val = DataLoader(dataset.val, batch_size = batch_size, shuffle = False)\n",
    "dataloader_test = DataLoader(dataset.test, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e4c515d-2a03-4d45-a520-dfc8b1294a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding = subwordembedding(num_embeddings = num_emb, \n",
    "                                  embedding_dim = emb_dim, \n",
    "                                  device = device, \n",
    "                                  padding_idx = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "023b4d66-8c19-41e2-9225-50837bf2e71c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                       | 1/900 [00:00<01:53,  7.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making instances (t,c,[ns])...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 900/900 [01:38<00:00,  9.12it/s]\n",
      "  0%|                                                                                               | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making instances (t,c,[ns])...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:03<00:00, 26.87it/s]\n",
      "  0%|                                                                                              | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making instances (t,c,[ns])...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 1000/1000 [01:41<00:00,  9.87it/s]\n"
     ]
    }
   ],
   "source": [
    "fasttext_train_dataset = FastTextDataset(word_dataset.train, context_size, neg_num, device, uniform)\n",
    "fasttext_val_dataset = FastTextDataset(word_dataset.val, context_size, neg_num, device, uniform)\n",
    "fasttext_test_dataset = FastTextDataset(word_dataset.test, context_size, neg_num, device, uniform)\n",
    "\n",
    "fasttext_loader_train = DataLoader(fasttext_train_dataset, batch_size = batch_size, shuffle = True)\n",
    "fasttext_loader_val = DataLoader(fasttext_val_dataset, batch_size = batch_size, shuffle = False)\n",
    "fasttext_loader_test = DataLoader(fasttext_test_dataset, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9714bbe-a8de-4713-9c3d-26c090fc4c4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "swe = subwordembedding(num_embeddings = num_emb, embedding_dim = emb_dim, device = device, padding_idx = 0, sumfirst = False)\n",
    "for batch in fasttext_loader_train:\n",
    "    target = batch['input']['target'].cpu()\n",
    "    print(swe(target).shape) \n",
    "    print(swe(target))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a739acc5-484f-4b63-b62a-33c7deedb735",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = pl.loggers.CSVLogger(\"../Trained_Models/SubwordEmbedding/logs\", name = f\"Fasttext_{emb_dim}_{dist}\")\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath = \"../Trained_Models/SubwordEmbedding/checkpoints\",\n",
    "    filename = 'best_model_{epoch}_loss-{val_loss:.2f}',\n",
    "    monitor = 'val_loss',\n",
    "    mode = 'min'\n",
    ")\n",
    "class LitProgressBar(pl.callbacks.ProgressBar):\n",
    "    def init_validation_tqdm(self):\n",
    "        bar = tqdm(disable=True)\n",
    "        return bar\n",
    "bar = LitProgressBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35a25234-21f8-4e56-b2b0-e25af649891a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\device_parser.py:130: LightningDeprecationWarning: Parsing of the Trainer argument gpus='0' (string) will change in the future. In the current version of Lightning, this will select CUDA device with index 0, but from v1.5 it will select gpus [] (same as gpus=0 (int)).\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type             | Params\n",
      "----------------------------------------------------\n",
      "0 | word_embedding | subwordembedding | 200 M \n",
      "----------------------------------------------------\n",
      "200 M     Trainable params\n",
      "0         Non-trainable params\n",
      "200 M     Total params\n",
      "800.000   Total estimated model params size (MB)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:102: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:102: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "907336ee5b2b4cfe860423b1e1866726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fasttext_model = FastTextTrainer(word_embedding, device, debug = False)\n",
    "torch.cuda.empty_cache()\n",
    "trainer = pl.Trainer(logger = logger, \n",
    "                     gpus = '0', \n",
    "                     callbacks = [checkpoint, bar], \n",
    "                     num_sanity_val_steps = 0, \n",
    "                     auto_lr_find = True,\n",
    "                     max_epochs = max_epochs)\n",
    "# trainer = pl.Trainer(logger=logger, callbacks=[checkpoint, bar], max_epochs=100)\n",
    "trainer.fit(fasttext_model, \n",
    "            train_dataloader = fasttext_loader_train, \n",
    "            val_dataloaders = fasttext_loader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94bb33e0-9729-43aa-862f-736401d7c0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:102: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb31d8e1ffc24239a76c2fa95e19d1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 186.8179168701172}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 186.8179168701172}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(test_dataloaders = fasttext_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8f4df7d-8753-4b1a-920e-dfa02ee0905d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(word_embedding.state_dict(), emb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43052e73-9ec6-4cfb-95e4-d2478aba4e24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best_model_path = checkpoint.best_model_path\n",
    "# checkpoint.best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e37128ce-d556-4b62-be24-5149d4c36a28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cp = torch.load(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2676acfe-c56e-4e41-9b78-7853eab066dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cp['state_dict']['word_embedding.subword_embedding.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b853aa9d-03e7-4f1c-afaf-be65ce9cb9fe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# word_embedding.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b71a88-9d68-47ab-840a-317c3cad3ea8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fa3e2ef-1343-4f8b-bfe0-8013efd6d09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2070'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21d65fc8-423e-497a-884d-05124e11a66b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "f4508ce6-659c-41f7-9fcd-c582eabb809f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "5048288e-cef2-4859-953a-1fefcdd795a5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61453812-6658-43e7-9cc8-9aa0cf0bc10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([1,4,10,55,12,13,22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e03b9-bc70-4a71-813d-bfdbfddc8477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "619223dc-e8bf-421d-a5a2-21be3314d561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = {1:2, 2:3}\n",
    "sum(list(g.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceee23a7-2b91-45cc-b0a9-3e04d39e30f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a444996-ef22-40c8-8661-133ddeb4797c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class test(Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.embed = Embedding(20, 5)\n",
    "#         self.embed.weight.data.uniform_(-0.05,0.05)\n",
    "        \n",
    "#     def forward(self):\n",
    "#         idx = torch.tensor([0,1], dtype = torch.long)\n",
    "#         multiplier = self.embed(idx)\n",
    "#         target = torch.tensor([[i for i in range(5)]], dtype = torch.long)\n",
    "#         print(\"idx.shape: \", idx.shape)\n",
    "#         print(\"idx: \", idx)\n",
    "#         print(\"\\nmultiplier.shape: \", multiplier.shape)\n",
    "#         print(\"multiplier: \", multiplier)\n",
    "#         print(\"\\ntarget.shape: \", target.shape)\n",
    "#         print(\"target: \", target)\n",
    "        \n",
    "#         product = torch.mul(target, multiplier)\n",
    "#         print(\"\\nproduct.shape: \", product.shape)\n",
    "#         print(\"product: \", product)\n",
    "#         emb_sum = torch.sum(product, dim=1)\n",
    "#         print(\"\\nsum.shape: \", emb_sum.shape)\n",
    "#         print(\"sum: \", emb_sum)\n",
    "        \n",
    "#         noise_dist = torch.ones(20)\n",
    "#         ng = torch.multinomial(noise_dist,5, replacement = True)\n",
    "        \n",
    "#         return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94cfa918-bd44-459f-8c8e-04b1c2483140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(int(True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ae26383-d2c5-429d-b58d-ac0b872af98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n100d\\n- uniform: 140 test\\n- noise: \\n50d\\n- uniform 127 test\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "100d\n",
    "- uniform: 140 test\n",
    "- noise: \n",
    "50d\n",
    "- uniform 127 test\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e47259-230f-4f30-b07a-a8007d288163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d032d0-4187-4629-8b0c-5b3f8b530d43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
